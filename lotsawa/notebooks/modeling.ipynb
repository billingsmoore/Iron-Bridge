{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tibetan to English Translation\n",
    "## Setup\n",
    "\n",
    "In this notebook, I will create a model to translate Tibetan sentences into English sentences. To create this model, I drew on the Keras tutorial provided here:\n",
    "\n",
    "https://keras.io/examples/nlp/neural_machine_translation_with_keras_nlp/\n",
    "\n",
    "I've adapted the model from the tutorial to translate Tibetan into English, rather than English to Spanish, and streamlined the code for simplicity and to meet my need for computational efficiency. Additionally, I've substantially altered the model in order to more fully optimize for the much, much smaller dataset available for the Tibetan language.\n",
    "\n",
    "The first step of this process is to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_nlp\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "TF_GPU_ALLOCATOR = 'cuda_malloc_async'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 1000\n",
    "MAX_SEQUENCE_LENGTH = 45\n",
    "VOCAB_SIZE = 15000\n",
    "\n",
    "EMBED_DIM = 256\n",
    "INTERMEDIATE_DIM = 2048\n",
    "NUM_HEADS = 8\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working With Sentence Pairs\n",
    "\n",
    "I've split the sentence pairs into two sets and set every English letter to be lowercase to avoid any confusion in the model. This is not necessary for Tibetan because the script does not use upper and lower cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = pathlib.Path('/home/j/Documents/Projects/Iron-Bridge/lotsawa/data/training-batches/training-batch-1.txt')\n",
    "\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    try:\n",
    "        tib, eng = line.split(\",\")[:2]\n",
    "        eng = eng.lower()\n",
    "        text_pairs.append((tib, eng))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499987 total pairs\n",
      "474988 training pairs\n",
      "24999 validation pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.05 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Tokenizer\n",
    "\n",
    "The tokenizer will assign each unique word in the dataset a 'token' a unique number that allows the data to be treated numerically during model training. In order to do this, a \"vocabulary\" must first be created. This is a complete list of the unique English and Tibetan words in the dataset.\n",
    "\n",
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def train_word_piece(text_samples, vocab_size, reserved_tokens):\\n    word_piece_ds = tf.data.Dataset.from_tensor_slices(text_samples)\\n    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\\n        word_piece_ds.batch(1000).prefetch(2),\\n        vocabulary_size=vocab_size,\\n        reserved_tokens=reserved_tokens,\\n    )\\n\\n    return vocab '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
    "    word_piece_ds = tf.data.Dataset.from_tensor_slices(text_samples)\n",
    "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "        word_piece_ds.batch(1000).prefetch(2),\n",
    "        vocabulary_size=vocab_size,\n",
    "        reserved_tokens=reserved_tokens,\n",
    "    )\n",
    "\n",
    "    return vocab \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Note that I've set aside some peculiar tokens. These correspond to whitespace,unknown characters, the beginnings and endings of sentences. I don't want the tokenizer to treat these things as words that need to be tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\\n\\ntib_samples = [text_pair[0] for text_pair in train_pairs]\\ntib_vocab = train_word_piece(tib_samples, VOCAB_SIZE, reserved_tokens)\\n\\neng_samples = [text_pair[1] for text_pair in train_pairs]\\neng_vocab = train_word_piece(eng_samples, VOCAB_SIZE, reserved_tokens) '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "tib_samples = [text_pair[0] for text_pair in train_pairs]\n",
    "tib_vocab = train_word_piece(tib_samples, VOCAB_SIZE, reserved_tokens)\n",
    "\n",
    "eng_samples = [text_pair[1] for text_pair in train_pairs]\n",
    "eng_vocab = train_word_piece(eng_samples, VOCAB_SIZE, reserved_tokens) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can tokenize the vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' eng_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\\n    vocabulary=eng_vocab, lowercase=False\\n)\\n\\ntib_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\\n    vocabulary=tib_vocab, lowercase=False\\n) '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" eng_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=eng_vocab, lowercase=False\n",
    ")\n",
    "\n",
    "tib_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=tib_vocab, lowercase=False\n",
    ") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I'll save the tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" with open('/home/j/Documents/Projects/Iron-Bridge/lotsawa/tokenizers/eng-tokenizer.pickle', 'wb') as handle:\\n    pickle.dump(eng_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\\n\\nwith open('/home/j/Documents/Projects/Iron-Bridge/lotsawa/tokenizers/tib-tokenizer.pickle', 'wb') as handle:\\n    pickle.dump(tib_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL) \""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" with open('/home/j/Documents/Projects/Iron-Bridge/lotsawa/tokenizers/eng-tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(eng_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('/home/j/Documents/Projects/Iron-Bridge/lotsawa/tokenizers/tib-tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tib_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/j/Documents/Projects/Iron-Bridge/lotsawa/tokenizers/tib-tokenizer.pickle', 'rb') as handle:\n",
    "    tib_tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('/home/j/Documents/Projects/Iron-Bridge/lotsawa/tokenizers/eng-tokenizer.pickle', 'rb') as handle:\n",
    "    eng_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Next, I will preprocess each batch of data. This consists of re-assembling the English-Tibetan sentence pairs. Each sentence must be padded with the \"[PAD]\" whitespace token in order to make each sequence of tokens the same length. This is because the model expects inputs of a particular shape. Once the sentence has been padded to the appropriate length, a [START] token can be appended to the beginning and an [END] token appended to the end.\n",
    "\n",
    "Finally, this assembled dataset can be split into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tib_eng_preprocess_batch(tib, eng):\n",
    "\n",
    "    tib = tib_tokenizer(tib)\n",
    "    eng = eng_tokenizer(eng)\n",
    "    \n",
    "\n",
    "    # add special tokens [start] and [end] and pad tib\n",
    "    tib_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length = MAX_SEQUENCE_LENGTH,\n",
    "        start_value = tib_tokenizer.token_to_id(\"[START]\"),\n",
    "        end_value = tib_tokenizer.token_to_id(\"[END]\"),\n",
    "        pad_value = tib_tokenizer.token_to_id(\"[PAD]\")\n",
    "    )\n",
    "\n",
    "    tib = tib_start_end_packer(tib)\n",
    "\n",
    "    # pad eng to max_sequence_length\n",
    "    eng_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH+1,\n",
    "        pad_value = eng_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "\n",
    "    eng = eng_start_end_packer(eng)\n",
    "\n",
    "\n",
    "\n",
    "    return (\n",
    "        {\n",
    "        \"encoder_inputs\": tib,\n",
    "        \"decoder_inputs\": eng[:, :-1]\n",
    "        },\n",
    "        eng[:, 1:],\n",
    "    )\n",
    "\n",
    "def make_dataset(pairs, batch_size=BATCH_SIZE):\n",
    "    tib_texts, eng_texts = zip(*pairs)\n",
    "    tib_texts = list(tib_texts)\n",
    "    eng_texts = list(eng_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((tib_texts, eng_texts))\n",
    "    dataset=dataset.batch(batch_size)\n",
    "    dataset = dataset.map(tib_eng_preprocess_batch, num_parallel_calls=AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "tib_eng_train_ds = make_dataset(train_pairs)\n",
    "tib_eng_val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model\n",
    "\n",
    "Now it's time to build the model itself. This model is an Autoencoder, which consists of an encoder and a decoder. \n",
    "\n",
    "The encoder input layer takes in a set of tokenized inputs. These inputs are then passed to a layer that accounts for the number assigned to the token as well as the position of that token in the sentence. The next layer is a typical dense Encoder layer.\n",
    "\n",
    "The decoder takes in a set of tokenized inputs from the Tibetan dataset and passes them to a layer that will account for the token number and position of the token in those sentences. This is then passed to a typical dense Decoder layer.\n",
    "\n",
    "Both the Encoder and Decoder layers are helpfully provided out-of-the-box by Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length = MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")(encoder_inputs)\n",
    "\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    intermediate_dim = INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(inputs=x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")(decoder_inputs)\n",
    "\n",
    "x = keras_nlp.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "decoder = keras.Model(\n",
    "    [\n",
    "        decoder_inputs,\n",
    "        encoded_seq_inputs\n",
    "    ],\n",
    "    decoder_outputs,\n",
    ")\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "tib_eng_translator = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_outputs,\n",
    "    name=\"tib_eng_translator\",\n",
    ") \"\"\"\n",
    "\n",
    "tib_eng_translator = tf.keras.models.load_model(\"/home/j/Documents/Projects/Iron-Bridge/lotsawa/models/tib-eng-translator-0.4.0.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tib_eng_translator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " token_and_position_embeddi  (None, None, 256)            3851520   ['encoder_inputs[0][0]']      \n",
      " ng (TokenAndPositionEmbedd                                                                       \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " transformer_encoder (Trans  (None, None, 256)            1315072   ['token_and_position_embedding\n",
      " formerEncoder)                                                     [0][0]']                      \n",
      "                                                                                                  \n",
      " model_1 (Functional)        (None, None, 15000)          9285272   ['decoder_inputs[0][0]',      \n",
      "                                                                     'transformer_encoder[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14451864 (55.13 MB)\n",
      "Trainable params: 14451864 (55.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tib_eng_translator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation\n",
    "\n",
    "Now, I've compiled the model.\n",
    "\n",
    "Of note here is the choice of optimization algorith. I have used RMSProp. RMSProp is similar to Adagrad, which we studied in class, and as a result it converges much more quickly than, say, SGD. However, it is less susceptible to vanishing gradients. This is perfect for our small dataset with small batch sizes.\n",
    "\n",
    "The loss function is Sparse Categorical Crossentropy. Not every word appears in every sentence so the data for most natural language related tasks is necessarily sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tib_eng_translator.compile(\n",
    "    \"rmsprop\", \n",
    "    loss=\"sparse_categorical_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "3711/3711 [==============================] - 220s 57ms/step - loss: 1.3497 - accuracy: 0.8280 - val_loss: 0.9398 - val_accuracy: 0.8773\n",
      "Epoch 2/1000\n",
      "3711/3711 [==============================] - 197s 53ms/step - loss: 1.0669 - accuracy: 0.8539 - val_loss: 0.8241 - val_accuracy: 0.8908\n",
      "Epoch 3/1000\n",
      "3711/3711 [==============================] - 200s 54ms/step - loss: 0.9687 - accuracy: 0.8645 - val_loss: 0.7606 - val_accuracy: 0.8980\n",
      "Epoch 4/1000\n",
      "3711/3711 [==============================] - 200s 54ms/step - loss: 0.9082 - accuracy: 0.8716 - val_loss: 0.7152 - val_accuracy: 0.9055\n",
      "Epoch 5/1000\n",
      "3711/3711 [==============================] - 200s 54ms/step - loss: 0.8668 - accuracy: 0.8768 - val_loss: 0.6863 - val_accuracy: 0.9107\n",
      "Epoch 6/1000\n",
      "3711/3711 [==============================] - 200s 54ms/step - loss: 0.8360 - accuracy: 0.8807 - val_loss: 0.6668 - val_accuracy: 0.9131\n",
      "Epoch 7/1000\n",
      "3711/3711 [==============================] - 201s 54ms/step - loss: 0.8098 - accuracy: 0.8839 - val_loss: 0.6446 - val_accuracy: 0.9159\n",
      "Epoch 8/1000\n",
      "3711/3711 [==============================] - 200s 54ms/step - loss: 0.7876 - accuracy: 0.8868 - val_loss: 0.6271 - val_accuracy: 0.9187\n",
      "Epoch 9/1000\n",
      "3711/3711 [==============================] - 200s 54ms/step - loss: 0.7679 - accuracy: 0.8893 - val_loss: 0.6055 - val_accuracy: 0.9218\n",
      "Epoch 10/1000\n",
      "3711/3711 [==============================] - 200s 54ms/step - loss: 0.7482 - accuracy: 0.8916 - val_loss: 0.5938 - val_accuracy: 0.9234\n",
      "Epoch 11/1000\n",
      "3711/3711 [==============================] - 200s 54ms/step - loss: 0.7322 - accuracy: 0.8937 - val_loss: 0.5813 - val_accuracy: 0.9247\n",
      "Epoch 12/1000\n",
      "3711/3711 [==============================] - 200s 54ms/step - loss: 0.7171 - accuracy: 0.8955 - val_loss: 0.5701 - val_accuracy: 0.9260\n",
      "Epoch 13/1000\n",
      "3711/3711 [==============================] - 200s 54ms/step - loss: 0.7014 - accuracy: 0.8972 - val_loss: 0.5558 - val_accuracy: 0.9283\n",
      "Epoch 14/1000\n",
      "3711/3711 [==============================] - 200s 54ms/step - loss: 0.6866 - accuracy: 0.8989 - val_loss: 0.5439 - val_accuracy: 0.9300\n",
      "Epoch 15/1000\n",
      "3711/3711 [==============================] - 199s 54ms/step - loss: 0.6742 - accuracy: 0.9004 - val_loss: 0.5308 - val_accuracy: 0.9310\n",
      "Epoch 16/1000\n",
      "3711/3711 [==============================] - 196s 53ms/step - loss: 0.6635 - accuracy: 0.9018 - val_loss: 0.5232 - val_accuracy: 0.9323\n",
      "Epoch 17/1000\n",
      "3711/3711 [==============================] - 188s 51ms/step - loss: 0.6526 - accuracy: 0.9032 - val_loss: 0.5186 - val_accuracy: 0.9330\n",
      "Epoch 18/1000\n",
      "3711/3711 [==============================] - 189s 51ms/step - loss: 0.6418 - accuracy: 0.9046 - val_loss: 0.5069 - val_accuracy: 0.9343\n",
      "Epoch 19/1000\n",
      "3711/3711 [==============================] - 199s 54ms/step - loss: 0.6344 - accuracy: 0.9057 - val_loss: 0.5017 - val_accuracy: 0.9348\n",
      "Epoch 20/1000\n",
      "3711/3711 [==============================] - 197s 53ms/step - loss: 0.6244 - accuracy: 0.9067 - val_loss: 0.4905 - val_accuracy: 0.9363\n",
      "Epoch 21/1000\n",
      "3711/3711 [==============================] - 192s 52ms/step - loss: 0.6170 - accuracy: 0.9078 - val_loss: 0.4892 - val_accuracy: 0.9360\n",
      "Epoch 22/1000\n",
      "3711/3711 [==============================] - 197s 53ms/step - loss: 0.6105 - accuracy: 0.9087 - val_loss: 0.4806 - val_accuracy: 0.9380\n",
      "Epoch 23/1000\n",
      " 175/3711 [>.............................] - ETA: 3:06 - loss: 0.6057 - accuracy: 0.9097"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/j/Documents/Projects/Iron-Bridge/lotsawa/notebooks/modeling.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/j/Documents/Projects/Iron-Bridge/lotsawa/notebooks/modeling.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tib_eng_history \u001b[39m=\u001b[39m tib_eng_translator\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/j/Documents/Projects/Iron-Bridge/lotsawa/notebooks/modeling.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     tib_eng_train_ds, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/j/Documents/Projects/Iron-Bridge/lotsawa/notebooks/modeling.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mEPOCHS, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/j/Documents/Projects/Iron-Bridge/lotsawa/notebooks/modeling.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mtib_eng_val_ds,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/j/Documents/Projects/Iron-Bridge/lotsawa/notebooks/modeling.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[acc_callback]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/j/Documents/Projects/Iron-Bridge/lotsawa/notebooks/modeling.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[1;32m    869\u001b[0m   )\n\u001b[1;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:138\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49munpack_inputs(bound_args)\n\u001b[1;32m    139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39mfunction\u001b[39m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/core/function/polymorphism/function_type.py:384\u001b[0m, in \u001b[0;36mFunctionType.unpack_inputs\u001b[0;34m(self, bound_parameters)\u001b[0m\n\u001b[1;32m    381\u001b[0m flat \u001b[39m=\u001b[39m []\n\u001b[1;32m    382\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m sorted_parameters:\n\u001b[1;32m    383\u001b[0m   flat\u001b[39m.\u001b[39mextend(\n\u001b[0;32m--> 384\u001b[0m       p\u001b[39m.\u001b[39;49mtype_constraint\u001b[39m.\u001b[39;49m_to_tensors(bound_parameters\u001b[39m.\u001b[39;49marguments[p\u001b[39m.\u001b[39;49mname])  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    385\u001b[0m   )\n\u001b[1;32m    387\u001b[0m dealiased_inputs \u001b[39m=\u001b[39m []\n\u001b[1;32m    388\u001b[0m ids_used \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/type_spec.py:251\u001b[0m, in \u001b[0;36mTypeSpec._to_tensors\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_tensors\u001b[39m(\u001b[39mself\u001b[39m, value):\n\u001b[1;32m    248\u001b[0m   tensors \u001b[39m=\u001b[39m []\n\u001b[1;32m    249\u001b[0m   nest\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m    250\u001b[0m       \u001b[39mlambda\u001b[39;00m spec, v: tensors\u001b[39m.\u001b[39mextend(spec\u001b[39m.\u001b[39m_to_tensors(v)),  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_component_specs,\n\u001b[1;32m    252\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_components(value))\n\u001b[1;32m    253\u001b[0m   \u001b[39mreturn\u001b[39;00m tensors\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:933\u001b[0m, in \u001b[0;36mIteratorSpec._component_specs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    932\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_component_specs\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 933\u001b[0m   \u001b[39mreturn\u001b[39;00m (tensor\u001b[39m.\u001b[39;49mTensorSpec([], dtypes\u001b[39m.\u001b[39;49mresource),)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/tensor.py:867\u001b[0m, in \u001b[0;36mDenseSpec.__init__\u001b[0;34m(self, shape, dtype, name)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a TensorSpec.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \n\u001b[1;32m    857\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[39m    not convertible to a `tf.DType`.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    866\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape \u001b[39m=\u001b[39m tensor_shape\u001b[39m.\u001b[39mTensorShape(shape)\n\u001b[0;32m--> 867\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39;49mas_dtype(dtype)\n\u001b[1;32m    868\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tib_eng_history = tib_eng_translator.fit(\n",
    "    tib_eng_train_ds, \n",
    "    epochs=EPOCHS, \n",
    "    validation_data=tib_eng_val_ds,\n",
    "    callbacks=[acc_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tib_eng_translator.save('/home/j/Documents/Projects/Iron-Bridge/lotsawa/models/tib-eng-translator-0.4.0.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tib_eng_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/j/Documents/Projects/Iron-Bridge/lotsawa/notebooks/modeling.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/j/Documents/Projects/Iron-Bridge/lotsawa/notebooks/modeling.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m acc \u001b[39m=\u001b[39m tib_eng_history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/j/Documents/Projects/Iron-Bridge/lotsawa/notebooks/modeling.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m val_acc \u001b[39m=\u001b[39m tib_eng_history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/j/Documents/Projects/Iron-Bridge/lotsawa/notebooks/modeling.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m loss \u001b[39m=\u001b[39m tib_eng_history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tib_eng_history' is not defined"
     ]
    }
   ],
   "source": [
    "acc = tib_eng_history.history['accuracy']\n",
    "val_acc = tib_eng_history.history['val_accuracy']\n",
    "\n",
    "loss = tib_eng_history.history['loss']\n",
    "val_loss = tib_eng_history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(tib_eng_history.history['loss']))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
