{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating Tibetan to English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "First, we'll import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 20:53:24.784054: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-12 20:53:24.784101: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-12 20:53:24.784146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-12 20:53:24.794340: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll load in the trained tokenizers and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 20:53:55.182418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 20:53:55.189733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 20:53:55.190041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 20:53:55.191287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 20:53:55.191587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 20:53:55.191828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 20:53:55.330060: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 20:53:55.330342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 20:53:55.330583: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 20:53:55.330778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2023-10-12 20:53:55.351736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 78.44MiB (82247680 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-10-12 20:53:55.352305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 70.59MiB (74022912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-10-12 20:53:55.353225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 63.53MiB (66620672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-10-12 20:53:55.354178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 57.18MiB (59958784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
     ]
    }
   ],
   "source": [
    "with open('../tokenizers/eng-tokenizer.pickle', 'rb') as handle:\n",
    "    eng_tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('../tokenizers/tib-tokenizer.pickle', 'rb') as handle:\n",
    "    tib_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 20:54:27.222468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 26.97MiB (28284672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-10-12 20:54:27.223685: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 26.97MiB (28284672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-10-12 20:54:37.225332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 26.97MiB (28284672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-10-12 20:54:37.226597: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 26.97MiB (28284672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-10-12 20:54:37.226648: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 14.65MiB (rounded to 15360000)requested by op Mul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-10-12 20:54:37.226663: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2023-10-12 20:54:37.226679: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 9, Chunks in use: 9. 2.2KiB allocated for chunks. 2.2KiB in use in bin. 52B client-requested in use in bin.\n",
      "2023-10-12 20:54:37.226692: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-12 20:54:37.226707: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 6, Chunks in use: 6. 6.2KiB allocated for chunks. 6.2KiB in use in bin. 6.0KiB client-requested in use in bin.\n",
      "2023-10-12 20:54:37.226721: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-12 20:54:37.226737: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-12 20:54:37.226755: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 1, Chunks in use: 1. 8.0KiB allocated for chunks. 8.0KiB in use in bin. 8.0KiB client-requested in use in bin.\n",
      "2023-10-12 20:54:37.226778: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 2, Chunks in use: 1. 46.0KiB allocated for chunks. 20.0KiB in use in bin. 20.0KiB client-requested in use in bin.\n",
      "2023-10-12 20:54:37.226793: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-12 20:54:37.226809: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-12 20:54:37.226827: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 1, Chunks in use: 1. 153.0KiB allocated for chunks. 153.0KiB in use in bin. 152.9KiB client-requested in use in bin.\n",
      "2023-10-12 20:54:37.226847: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 1, Chunks in use: 1. 266.2KiB allocated for chunks. 266.2KiB in use in bin. 266.0KiB client-requested in use in bin.\n",
      "2023-10-12 20:54:37.226865: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-12 20:54:37.226881: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-12 20:54:37.226899: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 2, Chunks in use: 2. 4.00MiB allocated for chunks. 4.00MiB in use in bin. 4.00MiB client-requested in use in bin.\n",
      "2023-10-12 20:54:37.226916: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 0. 4.00MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-12 20:54:37.226933: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-12 20:54:37.226951: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 2, Chunks in use: 2. 42.99MiB allocated for chunks. 42.99MiB in use in bin. 29.30MiB client-requested in use in bin.\n",
      "2023-10-12 20:54:37.226967: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-12 20:54:37.227004: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-12 20:54:37.227024: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-12 20:54:37.227040: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-12 20:54:37.227057: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 14.65MiB was 8.00MiB, Chunk State: \n",
      "2023-10-12 20:54:37.227073: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 53963008\n",
      "2023-10-12 20:54:37.227089: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1138000000 of size 156672 next 1\n",
      "2023-10-12 20:54:37.227107: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1138026400 of size 1280 next 2\n",
      "2023-10-12 20:54:37.227122: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1138026900 of size 272640 next 3\n",
      "2023-10-12 20:54:37.227136: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1138069200 of size 256 next 4\n",
      "2023-10-12 20:54:37.227150: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1138069300 of size 256 next 5\n",
      "2023-10-12 20:54:37.227164: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1138069400 of size 256 next 8\n",
      "2023-10-12 20:54:37.227177: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1138069500 of size 1024 next 6\n",
      "2023-10-12 20:54:37.227191: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1138069900 of size 256 next 7\n",
      "2023-10-12 20:54:37.227210: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1138069a00 of size 1024 next 9\n",
      "2023-10-12 20:54:37.227224: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1138069e00 of size 1024 next 13\n",
      "2023-10-12 20:54:37.227237: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f113806a200 of size 1024 next 14\n",
      "2023-10-12 20:54:37.227251: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f113806a600 of size 256 next 15\n",
      "2023-10-12 20:54:37.227269: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f113806a700 of size 256 next 16\n",
      "2023-10-12 20:54:37.227283: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f113806a800 of size 8192 next 19\n",
      "2023-10-12 20:54:37.227296: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f113806c800 of size 1024 next 22\n",
      "2023-10-12 20:54:37.227309: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f113806cc00 of size 256 next 17\n",
      "2023-10-12 20:54:37.227323: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f113806cd00 of size 256 next 18\n",
      "2023-10-12 20:54:37.227337: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f113806ce00 of size 256 next 24\n",
      "2023-10-12 20:54:37.227350: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f113806cf00 of size 26624 next 11\n",
      "2023-10-12 20:54:37.227364: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1138073700 of size 20480 next 12\n",
      "2023-10-12 20:54:37.227378: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f1138078700 of size 4194304 next 21\n",
      "2023-10-12 20:54:37.227392: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1138478700 of size 2097152 next 20\n",
      "2023-10-12 20:54:37.227406: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1138678700 of size 2097152 next 23\n",
      "2023-10-12 20:54:37.227419: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1138878700 of size 22269952 next 10\n",
      "2023-10-12 20:54:37.227434: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f1139db5700 of size 22811136 next 18446744073709551615\n",
      "2023-10-12 20:54:37.227447: I tensorflow/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2023-10-12 20:54:37.227465: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 9 Chunks of size 256 totalling 2.2KiB\n",
      "2023-10-12 20:54:37.227480: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 5 Chunks of size 1024 totalling 5.0KiB\n",
      "2023-10-12 20:54:37.227496: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2023-10-12 20:54:37.227513: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 8192 totalling 8.0KiB\n",
      "2023-10-12 20:54:37.227529: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 20480 totalling 20.0KiB\n",
      "2023-10-12 20:54:37.227545: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 156672 totalling 153.0KiB\n",
      "2023-10-12 20:54:37.227562: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 272640 totalling 266.2KiB\n",
      "2023-10-12 20:54:37.227577: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 2097152 totalling 4.00MiB\n",
      "2023-10-12 20:54:37.227592: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 22269952 totalling 21.24MiB\n",
      "2023-10-12 20:54:37.227609: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 22811136 totalling 21.75MiB\n",
      "2023-10-12 20:54:37.227624: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 47.44MiB\n",
      "2023-10-12 20:54:37.227639: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 53963008 memory_limit_: 82247680 available bytes: 28284672 curr_region_allocation_bytes_: 164495360\n",
      "2023-10-12 20:54:37.227659: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                        82247680\n",
      "InUse:                        49742080\n",
      "MaxInUse:                     53962752\n",
      "NumAllocs:                          43\n",
      "MaxAllocSize:                 22811136\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-10-12 20:54:37.227677: W tensorflow/tsl/framework/bfc_allocator.cc:497] *_______*************************************xxxxxxxxxxxx******************************xxxxxxxxxxxxx\n",
      "2023-10-12 20:54:37.227700: W tensorflow/core/framework/op_kernel.cc:1827] RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/home/j/Documents/Projects/Iron-Bridge/lotsawa/notebooks/translating.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/j/Documents/Projects/Iron-Bridge/lotsawa/notebooks/translating.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tib_eng_translator \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39m\"\u001b[39;49m\u001b[39m../models/tib-eng-translator-0.2.0.keras\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/saving_api.py:254\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m kwargs:\n\u001b[1;32m    250\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    251\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe following argument(s) are not supported \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwith the native Keras format: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m         )\n\u001b[0;32m--> 254\u001b[0m     \u001b[39mreturn\u001b[39;00m saving_lib\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[39m=\u001b[39;49mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[39mcompile\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcompile\u001b[39;49m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[39m=\u001b[39;49msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[39m# Legacy case.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[39mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[39m.\u001b[39mload_model(\n\u001b[1;32m    263\u001b[0m     filepath, custom_objects\u001b[39m=\u001b[39mcustom_objects, \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcompile\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    264\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:281\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    278\u001b[0m             asset_store\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    280\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 281\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    282\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:246\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 246\u001b[0m     model \u001b[39m=\u001b[39m deserialize_keras_object(\n\u001b[1;32m    247\u001b[0m         config_dict, custom_objects, safe_mode\u001b[39m=\u001b[39;49msafe_mode\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    250\u001b[0m all_filenames \u001b[39m=\u001b[39m zf\u001b[39m.\u001b[39mnamelist()\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m _VARS_FNAME \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.h5\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m all_filenames:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:728\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m safe_mode_scope \u001b[39m=\u001b[39m SafeModeScope(safe_mode)\n\u001b[1;32m    727\u001b[0m \u001b[39mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[0;32m--> 728\u001b[0m     instance \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_config(inner_config)\n\u001b[1;32m    729\u001b[0m     build_config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbuild_config\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    730\u001b[0m     \u001b[39mif\u001b[39;00m build_config:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3306\u001b[0m, in \u001b[0;36mModel.from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   3298\u001b[0m revivable_as_functional \u001b[39m=\u001b[39m (\n\u001b[1;32m   3299\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m {functional\u001b[39m.\u001b[39mFunctional, Model}\n\u001b[1;32m   3300\u001b[0m     \u001b[39mor\u001b[39;00m argspec\u001b[39m.\u001b[39margs[\u001b[39m1\u001b[39m:] \u001b[39m==\u001b[39m functional_init_args\n\u001b[1;32m   3301\u001b[0m     \u001b[39mor\u001b[39;00m (argspec\u001b[39m.\u001b[39mvarargs \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m argspec\u001b[39m.\u001b[39mvarkw \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mkwargs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3302\u001b[0m )\n\u001b[1;32m   3303\u001b[0m \u001b[39mif\u001b[39;00m is_functional_config \u001b[39mand\u001b[39;00m revivable_as_functional:\n\u001b[1;32m   3304\u001b[0m     \u001b[39m# Revive Functional model\u001b[39;00m\n\u001b[1;32m   3305\u001b[0m     \u001b[39m# (but not Functional subclasses with a custom __init__)\u001b[39;00m\n\u001b[0;32m-> 3306\u001b[0m     inputs, outputs, layers \u001b[39m=\u001b[39m functional\u001b[39m.\u001b[39;49mreconstruct_from_config(\n\u001b[1;32m   3307\u001b[0m         config, custom_objects\n\u001b[1;32m   3308\u001b[0m     )\n\u001b[1;32m   3309\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\n\u001b[1;32m   3310\u001b[0m         inputs\u001b[39m=\u001b[39minputs, outputs\u001b[39m=\u001b[39moutputs, name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3311\u001b[0m     )\n\u001b[1;32m   3312\u001b[0m     functional\u001b[39m.\u001b[39mconnect_ancillary_layers(model, layers)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/functional.py:1493\u001b[0m, in \u001b[0;36mreconstruct_from_config\u001b[0;34m(config, custom_objects, created_layers)\u001b[0m\n\u001b[1;32m   1491\u001b[0m \u001b[39m# First, we create all layers and enqueue nodes to be processed\u001b[39;00m\n\u001b[1;32m   1492\u001b[0m \u001b[39mfor\u001b[39;00m layer_data \u001b[39min\u001b[39;00m config[\u001b[39m\"\u001b[39m\u001b[39mlayers\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m-> 1493\u001b[0m     process_layer(layer_data)\n\u001b[1;32m   1494\u001b[0m \u001b[39m# Then we process nodes in order of layer depth.\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m \u001b[39m# Nodes that cannot yet be processed (if the inbound node\u001b[39;00m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# does not yet exist) are re-enqueued, and the process\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# is repeated until all nodes are processed.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mwhile\u001b[39;00m unprocessed_nodes:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/functional.py:1474\u001b[0m, in \u001b[0;36mreconstruct_from_config.<locals>.process_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1471\u001b[0m     \u001b[39m# Instantiate layer.\u001b[39;00m\n\u001b[1;32m   1472\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m deserialize \u001b[39mas\u001b[39;00m deserialize_layer\n\u001b[0;32m-> 1474\u001b[0m     layer \u001b[39m=\u001b[39m deserialize_layer(layer_data, custom_objects\u001b[39m=\u001b[39;49mcustom_objects)\n\u001b[1;32m   1475\u001b[0m     created_layers[layer_name] \u001b[39m=\u001b[39m layer\n\u001b[1;32m   1477\u001b[0m node_count_by_layer[layer] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(_should_skip_first_node(layer))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/layers/serialization.py:276\u001b[0m, in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects, use_legacy_format)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m use_legacy_format:\n\u001b[1;32m    269\u001b[0m     \u001b[39mreturn\u001b[39;00m legacy_serialization\u001b[39m.\u001b[39mdeserialize_keras_object(\n\u001b[1;32m    270\u001b[0m         config,\n\u001b[1;32m    271\u001b[0m         module_objects\u001b[39m=\u001b[39mLOCAL\u001b[39m.\u001b[39mALL_OBJECTS,\n\u001b[1;32m    272\u001b[0m         custom_objects\u001b[39m=\u001b[39mcustom_objects,\n\u001b[1;32m    273\u001b[0m         printable_module_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlayer\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    274\u001b[0m     )\n\u001b[0;32m--> 276\u001b[0m \u001b[39mreturn\u001b[39;00m serialization_lib\u001b[39m.\u001b[39;49mdeserialize_keras_object(\n\u001b[1;32m    277\u001b[0m     config,\n\u001b[1;32m    278\u001b[0m     module_objects\u001b[39m=\u001b[39;49mLOCAL\u001b[39m.\u001b[39;49mALL_OBJECTS,\n\u001b[1;32m    279\u001b[0m     custom_objects\u001b[39m=\u001b[39;49mcustom_objects,\n\u001b[1;32m    280\u001b[0m     printable_module_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlayer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    281\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:609\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(module_objects[config], types\u001b[39m.\u001b[39mFunctionType):\n\u001b[1;32m    603\u001b[0m             \u001b[39mreturn\u001b[39;00m deserialize_keras_object(\n\u001b[1;32m    604\u001b[0m                 serialize_with_public_fn(\n\u001b[1;32m    605\u001b[0m                     module_objects[config], config, fn_module_name\n\u001b[1;32m    606\u001b[0m                 ),\n\u001b[1;32m    607\u001b[0m                 custom_objects\u001b[39m=\u001b[39mcustom_objects,\n\u001b[1;32m    608\u001b[0m             )\n\u001b[0;32m--> 609\u001b[0m         \u001b[39mreturn\u001b[39;00m deserialize_keras_object(\n\u001b[1;32m    610\u001b[0m             serialize_with_public_class(\n\u001b[1;32m    611\u001b[0m                 module_objects[config], inner_config\u001b[39m=\u001b[39;49minner_config\n\u001b[1;32m    612\u001b[0m             ),\n\u001b[1;32m    613\u001b[0m             custom_objects\u001b[39m=\u001b[39;49mcustom_objects,\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    616\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PLAIN_TYPES):\n\u001b[1;32m    617\u001b[0m     \u001b[39mreturn\u001b[39;00m config\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:728\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m safe_mode_scope \u001b[39m=\u001b[39m SafeModeScope(safe_mode)\n\u001b[1;32m    727\u001b[0m \u001b[39mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[0;32m--> 728\u001b[0m     instance \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_config(inner_config)\n\u001b[1;32m    729\u001b[0m     build_config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbuild_config\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    730\u001b[0m     \u001b[39mif\u001b[39;00m build_config:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3306\u001b[0m, in \u001b[0;36mModel.from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   3298\u001b[0m revivable_as_functional \u001b[39m=\u001b[39m (\n\u001b[1;32m   3299\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m {functional\u001b[39m.\u001b[39mFunctional, Model}\n\u001b[1;32m   3300\u001b[0m     \u001b[39mor\u001b[39;00m argspec\u001b[39m.\u001b[39margs[\u001b[39m1\u001b[39m:] \u001b[39m==\u001b[39m functional_init_args\n\u001b[1;32m   3301\u001b[0m     \u001b[39mor\u001b[39;00m (argspec\u001b[39m.\u001b[39mvarargs \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m argspec\u001b[39m.\u001b[39mvarkw \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mkwargs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3302\u001b[0m )\n\u001b[1;32m   3303\u001b[0m \u001b[39mif\u001b[39;00m is_functional_config \u001b[39mand\u001b[39;00m revivable_as_functional:\n\u001b[1;32m   3304\u001b[0m     \u001b[39m# Revive Functional model\u001b[39;00m\n\u001b[1;32m   3305\u001b[0m     \u001b[39m# (but not Functional subclasses with a custom __init__)\u001b[39;00m\n\u001b[0;32m-> 3306\u001b[0m     inputs, outputs, layers \u001b[39m=\u001b[39m functional\u001b[39m.\u001b[39;49mreconstruct_from_config(\n\u001b[1;32m   3307\u001b[0m         config, custom_objects\n\u001b[1;32m   3308\u001b[0m     )\n\u001b[1;32m   3309\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\n\u001b[1;32m   3310\u001b[0m         inputs\u001b[39m=\u001b[39minputs, outputs\u001b[39m=\u001b[39moutputs, name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3311\u001b[0m     )\n\u001b[1;32m   3312\u001b[0m     functional\u001b[39m.\u001b[39mconnect_ancillary_layers(model, layers)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/functional.py:1493\u001b[0m, in \u001b[0;36mreconstruct_from_config\u001b[0;34m(config, custom_objects, created_layers)\u001b[0m\n\u001b[1;32m   1491\u001b[0m \u001b[39m# First, we create all layers and enqueue nodes to be processed\u001b[39;00m\n\u001b[1;32m   1492\u001b[0m \u001b[39mfor\u001b[39;00m layer_data \u001b[39min\u001b[39;00m config[\u001b[39m\"\u001b[39m\u001b[39mlayers\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m-> 1493\u001b[0m     process_layer(layer_data)\n\u001b[1;32m   1494\u001b[0m \u001b[39m# Then we process nodes in order of layer depth.\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m \u001b[39m# Nodes that cannot yet be processed (if the inbound node\u001b[39;00m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# does not yet exist) are re-enqueued, and the process\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# is repeated until all nodes are processed.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mwhile\u001b[39;00m unprocessed_nodes:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/functional.py:1474\u001b[0m, in \u001b[0;36mreconstruct_from_config.<locals>.process_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1471\u001b[0m     \u001b[39m# Instantiate layer.\u001b[39;00m\n\u001b[1;32m   1472\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m deserialize \u001b[39mas\u001b[39;00m deserialize_layer\n\u001b[0;32m-> 1474\u001b[0m     layer \u001b[39m=\u001b[39m deserialize_layer(layer_data, custom_objects\u001b[39m=\u001b[39;49mcustom_objects)\n\u001b[1;32m   1475\u001b[0m     created_layers[layer_name] \u001b[39m=\u001b[39m layer\n\u001b[1;32m   1477\u001b[0m node_count_by_layer[layer] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(_should_skip_first_node(layer))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/layers/serialization.py:276\u001b[0m, in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects, use_legacy_format)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m use_legacy_format:\n\u001b[1;32m    269\u001b[0m     \u001b[39mreturn\u001b[39;00m legacy_serialization\u001b[39m.\u001b[39mdeserialize_keras_object(\n\u001b[1;32m    270\u001b[0m         config,\n\u001b[1;32m    271\u001b[0m         module_objects\u001b[39m=\u001b[39mLOCAL\u001b[39m.\u001b[39mALL_OBJECTS,\n\u001b[1;32m    272\u001b[0m         custom_objects\u001b[39m=\u001b[39mcustom_objects,\n\u001b[1;32m    273\u001b[0m         printable_module_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlayer\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    274\u001b[0m     )\n\u001b[0;32m--> 276\u001b[0m \u001b[39mreturn\u001b[39;00m serialization_lib\u001b[39m.\u001b[39;49mdeserialize_keras_object(\n\u001b[1;32m    277\u001b[0m     config,\n\u001b[1;32m    278\u001b[0m     module_objects\u001b[39m=\u001b[39;49mLOCAL\u001b[39m.\u001b[39;49mALL_OBJECTS,\n\u001b[1;32m    279\u001b[0m     custom_objects\u001b[39m=\u001b[39;49mcustom_objects,\n\u001b[1;32m    280\u001b[0m     printable_module_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlayer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    281\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:731\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m build_config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbuild_config\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    730\u001b[0m \u001b[39mif\u001b[39;00m build_config:\n\u001b[0;32m--> 731\u001b[0m     instance\u001b[39m.\u001b[39;49mbuild_from_config(build_config)\n\u001b[1;32m    732\u001b[0m compile_config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcompile_config\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    733\u001b[0m \u001b[39mif\u001b[39;00m compile_config:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/base_layer.py:2331\u001b[0m, in \u001b[0;36mLayer.build_from_config\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   2329\u001b[0m input_shape \u001b[39m=\u001b[39m config[\u001b[39m\"\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   2330\u001b[0m \u001b[39mif\u001b[39;00m input_shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2331\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild(input_shape)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras_nlp/src/layers/modeling/token_and_position_embedding.py:109\u001b[0m, in \u001b[0;36mTokenAndPositionEmbedding.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild\u001b[39m(\u001b[39mself\u001b[39m, input_shape):\n\u001b[1;32m    108\u001b[0m     input_shape \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(input_shape)\n\u001b[0;32m--> 109\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoken_embedding\u001b[39m.\u001b[39;49mbuild(input_shape)\n\u001b[1;32m    110\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding\u001b[39m.\u001b[39mbuild(input_shape \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_dim,))\n\u001b[1;32m    111\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras_nlp/src/layers/modeling/reversible_embedding.py:109\u001b[0m, in \u001b[0;36mReversibleEmbedding.build\u001b[0;34m(self, inputs_shape)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild\u001b[39m(\u001b[39mself\u001b[39m, inputs_shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 109\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mbuild(inputs_shape)\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtie_weights:\n\u001b[1;32m    112\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreverse_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_weight(\n\u001b[1;32m    113\u001b[0m             name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreverse_embeddings\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    114\u001b[0m             shape\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_dim),\n\u001b[1;32m    115\u001b[0m             initializer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings_initializer,\n\u001b[1;32m    116\u001b[0m             dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype,\n\u001b[1;32m    117\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/tf_utils.py:371\u001b[0m, in \u001b[0;36mshape_type_conversion.<locals>.wrapper\u001b[0;34m(instance, input_shape)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[39mif\u001b[39;00m input_shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     input_shape \u001b[39m=\u001b[39m convert_shapes(input_shape, to_tuples\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 371\u001b[0m output_shape \u001b[39m=\u001b[39m fn(instance, input_shape)\n\u001b[1;32m    372\u001b[0m \u001b[39m# Return shapes from `fn` as TensorShapes.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39mif\u001b[39;00m output_shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:179\u001b[0m, in \u001b[0;36mEmbedding.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39m@tf_utils\u001b[39m\u001b[39m.\u001b[39mshape_type_conversion\n\u001b[1;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild\u001b[39m(\u001b[39mself\u001b[39m, input_shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_weight(\n\u001b[1;32m    180\u001b[0m         shape\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_dim),\n\u001b[1;32m    181\u001b[0m         initializer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings_initializer,\n\u001b[1;32m    182\u001b[0m         name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39membeddings\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    183\u001b[0m         regularizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings_regularizer,\n\u001b[1;32m    184\u001b[0m         constraint\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings_constraint,\n\u001b[1;32m    185\u001b[0m         experimental_autocast\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    186\u001b[0m     )\n\u001b[1;32m    187\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/base_layer.py:712\u001b[0m, in \u001b[0;36mLayer.add_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[39mif\u001b[39;00m layout:\n\u001b[1;32m    710\u001b[0m     getter \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(getter, layout\u001b[39m=\u001b[39mlayout)\n\u001b[0;32m--> 712\u001b[0m variable \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_variable_with_custom_getter(\n\u001b[1;32m    713\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    714\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m    715\u001b[0m     \u001b[39m# TODO(allenl): a `make_variable` equivalent should be added as a\u001b[39;49;00m\n\u001b[1;32m    716\u001b[0m     \u001b[39m# `Trackable` method.\u001b[39;49;00m\n\u001b[1;32m    717\u001b[0m     getter\u001b[39m=\u001b[39;49mgetter,\n\u001b[1;32m    718\u001b[0m     \u001b[39m# Manage errors in Layer rather than Trackable.\u001b[39;49;00m\n\u001b[1;32m    719\u001b[0m     overwrite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    720\u001b[0m     initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[1;32m    721\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    722\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m    723\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    724\u001b[0m     use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[1;32m    725\u001b[0m     collections\u001b[39m=\u001b[39;49mcollections_arg,\n\u001b[1;32m    726\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m    727\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[1;32m    728\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m    729\u001b[0m )\n\u001b[1;32m    730\u001b[0m \u001b[39mif\u001b[39;00m regularizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[39m# TODO(fchollet): in the future, this should be handled at the\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[39m# level of variable creation, and weight regularization losses\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[39m# should be variable attributes.\u001b[39;00m\n\u001b[1;32m    734\u001b[0m     name_in_scope \u001b[39m=\u001b[39m variable\u001b[39m.\u001b[39mname[: variable\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:492\u001b[0m, in \u001b[0;36mTrackable._add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    482\u001b[0m   \u001b[39mif\u001b[39;00m (checkpoint_initializer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    483\u001b[0m       \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(initializer, CheckpointInitialValueCallable) \u001b[39mand\u001b[39;00m\n\u001b[1;32m    484\u001b[0m            (initializer\u001b[39m.\u001b[39mrestore_uid \u001b[39m>\u001b[39m checkpoint_initializer\u001b[39m.\u001b[39mrestore_uid))):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[39m# then we'll catch that when we call _track_trackable. So this is\u001b[39;00m\n\u001b[1;32m    490\u001b[0m     \u001b[39m# \"best effort\" to set the initializer with the highest restore UID.\u001b[39;00m\n\u001b[1;32m    491\u001b[0m     initializer \u001b[39m=\u001b[39m checkpoint_initializer\n\u001b[0;32m--> 492\u001b[0m new_variable \u001b[39m=\u001b[39m getter(\n\u001b[1;32m    493\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    494\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m    495\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    496\u001b[0m     initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[1;32m    497\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_for_getter)\n\u001b[1;32m    499\u001b[0m \u001b[39m# If we set an initializer and the variable processed it, tracking will not\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39m# assign again. It will add this variable to our dependencies, and if there\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m# is a non-trivial restoration queued, it will handle that. This also\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39m# handles slot variables.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m overwrite \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(new_variable, Trackable):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/base_layer_utils.py:137\u001b[0m, in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner, layout, experimental_enable_variable_lifting)\u001b[0m\n\u001b[1;32m    130\u001b[0m     use_resource \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39mif\u001b[39;00m layout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# In theory, in `use_resource` is True and `collections` is empty\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[39m# (that is to say, in TF2), we can use tf.Variable.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[39m# However, this breaks legacy (Estimator) checkpoints because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[39m# it changes variable names. Remove this when V1 is fully deprecated.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m tf1\u001b[39m.\u001b[39;49mVariable(\n\u001b[1;32m    138\u001b[0m         initial_value\u001b[39m=\u001b[39;49minit_val,\n\u001b[1;32m    139\u001b[0m         name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    140\u001b[0m         trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    141\u001b[0m         caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m    142\u001b[0m         dtype\u001b[39m=\u001b[39;49mvariable_dtype,\n\u001b[1;32m    143\u001b[0m         validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[1;32m    144\u001b[0m         constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m    145\u001b[0m         use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[1;32m    146\u001b[0m         collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[1;32m    147\u001b[0m         synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m    148\u001b[0m         aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[1;32m    149\u001b[0m         shape\u001b[39m=\u001b[39;49mvariable_shape \u001b[39mif\u001b[39;49;00m variable_shape \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    150\u001b[0m         experimental_enable_variable_lifting\u001b[39m=\u001b[39;49mexperimental_enable_variable_lifting,  \u001b[39m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    151\u001b[0m     )\n\u001b[1;32m    152\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     \u001b[39mreturn\u001b[39;00m dtensor\u001b[39m.\u001b[39mDVariable(\n\u001b[1;32m    154\u001b[0m         initial_value\u001b[39m=\u001b[39minit_val,\n\u001b[1;32m    155\u001b[0m         name\u001b[39m=\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m         shape\u001b[39m=\u001b[39mvariable_shape \u001b[39mif\u001b[39;00m variable_shape \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    165\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/initializers/initializers.py:356\u001b[0m, in \u001b[0;36mRandomUniform.__call__\u001b[0;34m(self, shape, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m     _ensure_keras_seeded()\n\u001b[1;32m    347\u001b[0m     \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39mcall_with_layout(\n\u001b[1;32m    348\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_random_generator\u001b[39m.\u001b[39mrandom_uniform,\n\u001b[1;32m    349\u001b[0m         layout,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m         nonce,\n\u001b[1;32m    355\u001b[0m     )\n\u001b[0;32m--> 356\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_random_generator\u001b[39m.\u001b[39;49mrandom_uniform(\n\u001b[1;32m    357\u001b[0m     shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminval, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmaxval, dtype, nonce\n\u001b[1;32m    358\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend.py:2102\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[0;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[1;32m   2100\u001b[0m     \u001b[39mif\u001b[39;00m nonce:\n\u001b[1;32m   2101\u001b[0m         seed \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[0;32m-> 2102\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mstateless_uniform(\n\u001b[1;32m   2103\u001b[0m         shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m   2104\u001b[0m         minval\u001b[39m=\u001b[39;49mminval,\n\u001b[1;32m   2105\u001b[0m         maxval\u001b[39m=\u001b[39;49mmaxval,\n\u001b[1;32m   2106\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   2107\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m   2108\u001b[0m     )\n\u001b[1;32m   2109\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\n\u001b[1;32m   2110\u001b[0m     shape\u001b[39m=\u001b[39mshape,\n\u001b[1;32m   2111\u001b[0m     minval\u001b[39m=\u001b[39mminval,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2114\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_legacy_seed(),\n\u001b[1;32m   2115\u001b[0m )\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: "
     ]
    }
   ],
   "source": [
    "tib_eng_translator = tf.keras.models.load_model(\"../models/tib-eng-translator-0.2.0.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to bring our constants back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 15\n",
    "VOCAB_SIZE = 15000\n",
    "\n",
    "EMBED_DIM = 256\n",
    "INTERMEDIATE_DIM = 2048\n",
    "NUM_HEADS = 8\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Translated Sentences\n",
    "\n",
    "Even if the translations are perfect, the outputs of our model are not meaningful sentences. The model only outputs numerical tokens. In order to turn these into something that a human can read they need to be decoded.\n",
    "\n",
    "Below is a function to decode these translated sentences. This function takes in an English sentence, runs it through our translator model then works its way through the output of the model, converting the output into words in Tibetan using our tokenizers.\n",
    "\n",
    "Part of decoding the sequence is sampling the probabilities of tokens that should follow the existing translation. The sampler is the algorithm that is used to select that next work or token. Here I've used the Greedy sampler which simply finds the highest likelihood next word and adds it to the translated sentence. It is computationally inexpensive and because the outputs are pretty short we don't need to worry about the Greedy sampler outputting long, repetitive sentences that don't make much sense, which can be an issue with the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tib_eng_translate(input_sentences):\n",
    "    batch_size = tf.shape(input_sentences)[0]\n",
    "\n",
    "    encoder_input_tokens = tib_tokenizer(input_sentences).to_tensor(\n",
    "        shape=(None, MAX_SEQUENCE_LENGTH)\n",
    "    )\n",
    "\n",
    "    def next(prompt, cache, index):\n",
    "        logits = tib_eng_translator([encoder_input_tokens, prompt])[:, index - 1, :]\n",
    "        hidden_states = None\n",
    "        return logits, hidden_states, cache\n",
    "    \n",
    "    length = MAX_SEQUENCE_LENGTH\n",
    "    start = tf.fill((batch_size, 1), eng_tokenizer.token_to_id(\"[START]\"))\n",
    "    pad = tf.fill((batch_size, length - 1), eng_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    prompt = tf.concat((start, pad), axis=-1)\n",
    "\n",
    "    generated_tokens = keras_nlp.samplers.GreedySampler()(\n",
    "        next,\n",
    "        prompt,\n",
    "        end_token_id=eng_tokenizer.token_to_id(\"[END]\"),\n",
    "        index=1\n",
    "    )\n",
    "    generated_sentences = eng_tokenizer.detokenize(generated_tokens)\n",
    "    try:\n",
    "        generated_sentences = generated_sentences.numpy()[0].decode(\"utf-8\")\n",
    "\n",
    "        generated_sentences = (\n",
    "            generated_sentences.replace(\"[PAD]\", \"\")\n",
    "            .replace(\"[START]\", \"\")\n",
    "            .replace(\"[END]\", \"\")\n",
    "            .replace(\"[UNK]\", \"\")\n",
    "            .strip()\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "    return generated_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Translations\n",
    "\n",
    "Now, let's look at some example translations from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Example **\n",
      "saltong zungjuk nyid teng\n",
      "p khon chok la  ng  dra\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'saltong zungjuk nyid teng'\n",
    "translated = tib_eng_translate(tf.constant([input_sentence]))\n",
    "\n",
    "print(f\"** Example **\")\n",
    "print(input_sentence)\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
