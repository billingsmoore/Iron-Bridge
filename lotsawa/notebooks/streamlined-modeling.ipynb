{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tibetan to English Translation\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_nlp\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "TF_GPU_ALLOCATOR = 'cuda_malloc_async'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 45\n",
    "VOCAB_SIZE = 15000\n",
    "\n",
    "EMBED_DIM = 256\n",
    "INTERMEDIATE_DIM = 2048\n",
    "NUM_HEADS = 8\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/j/Documents/Projects/Iron-Bridge/lotsawa/tokenizers/tib-tokenizer.pickle', 'rb') as handle:\n",
    "    tib_tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('/home/j/Documents/Projects/Iron-Bridge/lotsawa/tokenizers/eng-tokenizer.pickle', 'rb') as handle:\n",
    "    eng_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(text_file):\n",
    "    with open(text_file) as f:\n",
    "        lines = f.read().split(\"\\n\")[:-1]\n",
    "    text_pairs = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            tib, eng = line.split(\",\")[:2]\n",
    "            eng = eng.lower()\n",
    "            text_pairs.append((tib, eng))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    random.shuffle(text_pairs)\n",
    "    num_val_samples = int(0.05 * len(text_pairs))\n",
    "    num_train_samples = len(text_pairs) - num_val_samples\n",
    "    train_pairs = text_pairs[:num_train_samples]\n",
    "    val_pairs = text_pairs[num_train_samples :]\n",
    "\n",
    "    print(f\"{len(text_pairs)} total pairs\")\n",
    "    print(f\"{len(train_pairs)} training pairs\")\n",
    "    print(f\"{len(val_pairs)} validation pairs\")\n",
    "\n",
    "    return train_pairs, val_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tib_eng_preprocess_batch(tib, eng):\n",
    "\n",
    "    tib = tib_tokenizer(tib)\n",
    "    eng = eng_tokenizer(eng)\n",
    "    \n",
    "\n",
    "    # add special tokens [start] and [end] and pad tib\n",
    "    tib_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length = MAX_SEQUENCE_LENGTH,\n",
    "        start_value = tib_tokenizer.token_to_id(\"[START]\"),\n",
    "        end_value = tib_tokenizer.token_to_id(\"[END]\"),\n",
    "        pad_value = tib_tokenizer.token_to_id(\"[PAD]\")\n",
    "    )\n",
    "\n",
    "    tib = tib_start_end_packer(tib)\n",
    "\n",
    "    # pad eng to max_sequence_length\n",
    "    eng_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH+1,\n",
    "        pad_value = eng_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "\n",
    "    eng = eng_start_end_packer(eng)\n",
    "\n",
    "\n",
    "\n",
    "    return (\n",
    "        {\n",
    "        \"encoder_inputs\": tib,\n",
    "        \"decoder_inputs\": eng[:, :-1]\n",
    "        },\n",
    "        eng[:, 1:],\n",
    "    )\n",
    "\n",
    "def make_dataset(pairs, batch_size):\n",
    "    tib_texts, eng_texts = zip(*pairs)\n",
    "    tib_texts = list(tib_texts)\n",
    "    eng_texts = list(eng_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((tib_texts, eng_texts))\n",
    "    dataset=dataset.batch(batch_size)\n",
    "    dataset = dataset.map(tib_eng_preprocess_batch, num_parallel_calls=AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length = MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")(encoder_inputs)\n",
    "\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    intermediate_dim = INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(inputs=x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")(decoder_inputs)\n",
    "\n",
    "x = keras_nlp.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "decoder = keras.Model(\n",
    "    [\n",
    "        decoder_inputs,\n",
    "        encoded_seq_inputs\n",
    "    ],\n",
    "    decoder_outputs,\n",
    ")\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "tib_eng_translator = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_outputs,\n",
    "    name=\"tib_eng_translator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tib_eng_translator.compile(\n",
    "    \"rmsprop\", \n",
    "    loss=\"sparse_categorical_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "acc_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for i in range(31):\n",
    "        \n",
    "        # load in batch from training batches\n",
    "        text_file = '/home/j/Documents/Projects/Iron-Bridge/lotsawa/data/training-batches/training-batch-'+str(i)+'.txt'\n",
    "        train_pairs, val_pairs = load_data(text_file)\n",
    "        \n",
    "        for j in range(1, 6):\n",
    "            # make datasets from loaded data\n",
    "            batch_size = 16 * (2**j)\n",
    "            tib_eng_train_ds = make_dataset(train_pairs, batch_size)\n",
    "            tib_eng_val_ds = make_dataset(val_pairs, batch_size)\n",
    "            \n",
    "            # fit model to dataset batch\n",
    "            tib_eng_translator.fit(\n",
    "            tib_eng_train_ds, \n",
    "            epochs=100, \n",
    "            validation_data=tib_eng_val_ds,\n",
    "            callbacks=[acc_callback]\n",
    "            )\n",
    "\n",
    "            # save the model\n",
    "            tib_eng_translator.save('/home/j/Documents/Projects/Iron-Bridge/lotsawa/models/tib-eng-translator-0.4.1.'+str(i)+'.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
