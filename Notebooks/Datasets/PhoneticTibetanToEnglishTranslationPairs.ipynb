{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "The purpose of this notebook is to document the process that was used to create the initial data set for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping From Lotsawa House\n",
    "\n",
    "Data was scraped primarily from [Lotsawa House](https://www.lotsawahouse.org/)\n",
    "\n",
    "The code below is a selenium script which iteratively opens the 'topic' collections from Lotsawa House. Each topic typically has a full collection pdf available. However, that PDF is not available at a particular URL until it is requested by clicking the 'PDF' link on each topic page. \n",
    "\n",
    "This script clicks the link then redirects the browser to the URL of the PDF version of the collected texts for that topic. The script then downloads that PDF. Note that this code relies on an untracked '.txt' file that contains a list of topics featured on the website. The script also provides a certain amount of sanitization on the text of that list to account for non-standard characters and diacriticals common to transliteration of Tibetan and Sanskrit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def download_pdf(driver, topic):\n",
    "    driver.get('https://www.lotsawahouse.org/topics/'+topic+'/')\n",
    "\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    driver.execute_script(\"window.scrollTo(0, 400)\")\n",
    "\n",
    "\n",
    "    # click pdf link so url will be active\n",
    "    pdf_link = driver.find_element(By.LINK_TEXT, 'PDF')\n",
    "\n",
    "    pdf_link.click()\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    # download pdf from active url\n",
    "    url = 'https://www.lotsawahouse.org/Cgi/make-ebook-cgi.pl?lang=english&path=topics%2F'+topic+'&format=PDF&do=download'\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    with open('MLotsawa/data/lotsawahouse/topic-pdfs/'+topic+'.pdf', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "# window to webpage\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "topic_list = []\n",
    "remaining = []\n",
    "\n",
    "with open('MLotsawa/data/lotsawahouse/topic-list.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        topic = line.lower().replace(' ', '-').replace('&', '').replace('--', '-').replace('\\'', '')\n",
    "        topic = topic.replace('ā','a').replace('ḍ', 'd').replace('é', 'e').replace('ī','i').replace('ṃ','m').replace('ṇ', 'n').replace('ñ','n').replace('ö', 'o').replace('ś', 'sh').replace('ṣ','sh').replace('ü', 'u').replace('ū', 'u')\n",
    "        topic_list.append(topic)\n",
    "\n",
    "for topic in topic_list:\n",
    "    try:\n",
    "        download_pdf(driver, topic)\n",
    "    except:\n",
    "        remaining.append(topic)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "with open('MLotsawa/data/lotsawahouse/remaining-topics.txt', 'w') as f:\n",
    "    f.writelines(remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ankhi and Microsoft Translator\n",
    "\n",
    "A small proportion of the training data was sourced from common phrases for language learners provided from crowd sourcing on the language learning platorm Ankhi. This data is easily available online at present, however, there are no sets of English sentences paired with Tibetan. To attempt to put together a comparable set of sentence pairs, I programmatically translate English sentences from Ankhi datasets into Tibetan using Microsoft Translator. This process was relatively laborious and produced data of relatively poor quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, uuid, json\n",
    "import pyewts\n",
    "import time\n",
    "\n",
    "# open bing api key\n",
    "with open('bing-key.txt', 'r') as f:\n",
    "    key = f.read()\n",
    "key=key.replace('\\n', '')\n",
    "\n",
    "\n",
    "# this function passes sets of sentences to the microsoft translator api\n",
    "def translate(key, num, batch_num):\n",
    "    # Add your key and endpoint\n",
    "\n",
    "    endpoint = \"https://api.cognitive.microsofttranslator.com\"\n",
    "\n",
    "\n",
    "    # location, also known as region.\n",
    "    # required if you're using a multi-service or regional (not global) resource. It can be found in the Azure portal on the Keys and Endpoint page.\n",
    "    location = \"eastus\"\n",
    "\n",
    "    path = '/translate'\n",
    "    constructed_url = endpoint + path\n",
    "\n",
    "    params = {\n",
    "        'api-version': '3.0',\n",
    "        'from': 'en',\n",
    "        'to': 'bo',\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': key,\n",
    "        # location required if you're using a multi-service or regional (not global) resource.\n",
    "        'Ocp-Apim-Subscription-Region': location,\n",
    "        'Content-type': 'application/json',\n",
    "        'X-ClientTraceId': str(uuid.uuid4())\n",
    "    }\n",
    "\n",
    "    # You can pass more than one object in body.\n",
    "    body = []\n",
    "\n",
    "    with open('MLotsawa/data/ankhi/bing-batches/mini-batches/'+str(num)+'/'+str(batch_num)+'.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            body.append({'text': line})\n",
    "\n",
    "    request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
    "    response = request.json()\n",
    "    #print(json.dumps(response, sort_keys=True, ensure_ascii=False, indent=4, separators=(',', ': ')))\n",
    "    with open('MLotsawa/data/ankhi/bing-translations/batch'+str(num)+'-'+str(batch_num)+'-response.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(response, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Microsoft Translator provides translations into Tibetan in the Tibetan script. I converted this into Wylie transliterations with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transliterate(num, batch_num):\n",
    "    converter=pyewts.pyewts()\n",
    "\n",
    "    with open('MLotsawa/data/ankhi/bing-translations/batch'+str(num)+'-'+str(batch_num)+'-response.json') as f:\n",
    "        d = json.load(f)\n",
    "\n",
    "    tib = []\n",
    "\n",
    "    for entry in d:\n",
    "        tib.append(converter.toWylie(entry['translations'][0]['text']))\n",
    "\n",
    "    eng = []\n",
    "\n",
    "    with open('MLotsawa/data/ankhi/bing-batches/mini-batches/'+str(num)+'/'+str(batch_num)+'.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            eng.append(line)\n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    for i in range(len(tib)):\n",
    "        pairs.append((tib[i], eng[i]))\n",
    "\n",
    "    with open('MLotsawa/data/ankhi/pairs/batch1'+str(num)+'-'+str(batch_num)+'-pairs.txt', 'w') as f: # make sure to change to 'a'\n",
    "        f.writelines('\\n'.join(str(pair).replace('\\'', '')\n",
    "                                        .replace('\\\\n', '')\n",
    "                                        .replace('(', '')\n",
    "                                        .replace(')', '')\n",
    "                                        .replace('/', '')\n",
    "                                        .replace(' ,', ',')\n",
    "                                        .replace('\"', '')  for pair in pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process was summarized and performed iteratively to be saved as sentence pairs with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(109,119):\n",
    "    for i in range(0,10):\n",
    "        batch_num = i\n",
    "        translate(key, j, batch_num)\n",
    "        transliterate(j, batch_num)\n",
    "\n",
    "import os\n",
    "\n",
    "for file in os.listdir('/data/ankhi/pairs/'):\n",
    "    path = '/data/ankhi/pairs/' + file\n",
    "    with open(path, 'r') as f:\n",
    "        text = f.readlines()\n",
    "        with open('/data/ankhi/all-ankhi-pairs.txt', 'a') as g:\n",
    "            g.write('\\n')\n",
    "            g.writelines(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting PDFs to txt\n",
    "\n",
    "The translations come in bilingual pdfs which need to be converted to a usable .txt file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "\n",
    "path = '/home/j/Documents/Projects/Iron-Bridge/lotsawa/data/lotsawahouse/topic-pdfs'\n",
    "\n",
    "def pdf_to_txt(file):\n",
    "    reader = PdfReader(file)\n",
    "\n",
    "    num_pages = len(reader.pages)\n",
    "\n",
    "    text = []\n",
    "\n",
    "    for page in reader.pages:\n",
    "        text.append(page.extract_text())\n",
    "\n",
    "\n",
    "    return text\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    text = pdf_to_txt(path + '/' + file)\n",
    "    with open('/home/j/Documents/Projects/Iron-Bridge/lotsawa/data/lotsawahouse/topic-txts/' + file[:-4] + '.txt', 'w') as f:\n",
    "        f.writelines('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split text into sentence pairs\n",
    "\n",
    "Now that we've wittled the text down we can set the text into Tibetan and English sentence pairs. Lotsawa House translations are conveniently provided in multiple lines. First Tibetan and then the English translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_language_detection import LanguageDetector\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "nlp_model = spacy.load(\"en_core_web_md\")\n",
    "spacy.language.Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "nlp_model.add_pipe('language_detector', last=True)\n",
    "\n",
    "pairs = []\n",
    "\n",
    "def detect(text):\n",
    "    doc = nlp_model(text)\n",
    "    detect_language = doc._.language\n",
    "    lang = detect_language['language']\n",
    "    return lang\n",
    "\n",
    "def separate_pairs(file):\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.readlines()\n",
    "        \n",
    "        for i in range(len(text) - 1):\n",
    "            lang = detect(text[i])\n",
    "            if lang != \"en\":\n",
    "                next_lang = detect(text[i+1])\n",
    "                if next_lang == \"en\":\n",
    "                    pair = (text[i].replace('\\n', '') + ',' + text[i+1])\n",
    "                    pairs.append(pair)\n",
    "\n",
    "    with open('/home/j/Documents/Projects/Iron-Bridge/lotsawa/data/lotsawahouse/all-lotsawahouse-pairs.txt', 'a') as f:\n",
    "        f.write('\\n')\n",
    "        f.writelines(pairs)\n",
    "\n",
    "path = '/home/j/Documents/Projects/Iron-Bridge/lotsawa/data/lotsawahouse/topic-txts/'\n",
    "\n",
    "for file in os.listdir('/home/j/Documents/Projects/Iron-Bridge/lotsawa/data/lotsawahouse/topic-txts'):\n",
    "    separate_pairs(path+file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
