{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': 'ཐུབ་པས་རྟག་ཏུ་དེ་བཞིན་སྤྱད།།',\n",
       " 'Target': 'The aspirant should move in such a way at all times.',\n",
       " 'File_Name': 'TM2382',\n",
       " 'Machine Aligned': True,\n",
       " '__index_level_0__': 0,\n",
       " 'Tag': 'Prophecies, Rituals'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('openpecha/tagged_cleaned_MT_v1.0.3')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "\n",
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Blank Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition(example):\n",
    "    return example['Tag'] != ''\n",
    "\n",
    "ds = ds.filter(condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Source', 'Target', 'File_Name', 'Machine Aligned', '__index_level_0__', 'Tag'],\n",
       "        num_rows: 1163105\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Source', 'Target', 'File_Name', 'Machine Aligned', '__index_level_0__', 'Tag'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': 'ཐུབ་པས་རྟག་ཏུ་དེ་བཞིན་སྤྱད།།',\n",
       " 'Target': 'The aspirant should move in such a way at all times.',\n",
       " 'File_Name': 'TM2382',\n",
       " 'Machine Aligned': True,\n",
       " '__index_level_0__': 0,\n",
       " 'Tag': 'Prophecies, Rituals'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collapse Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "buddhist_labels = ['Mantras',\n",
    "                    'Dzogchen',\n",
    "                    'Astrology',\n",
    "                    'Monastery',\n",
    "                    'Mahamudra',\n",
    "                    'Mind',\n",
    "                    'Meditation',\n",
    "                    'Self, Logic, Aggregates',\n",
    "                    'Tantra',\n",
    "                    'Emptiness',\n",
    "                    'Dreams',\n",
    "                    'Education, Teaching',\n",
    "                    'Ethics, Enlightenment, Wisdom',\n",
    "                    'Prophecies, Rituals',\n",
    "                    'Lama',\n",
    "                    'Samsara, Nirvana',\n",
    "                    'Milarepa, Realization, Biography',\n",
    "                    'Kayas',\n",
    "                    'Intrinsic Existence, Conventional Existence',\n",
    "                    'Time, Causality, Perception',\n",
    "                    'Natural State',\n",
    "                    'Karma, Consequences',\n",
    "                    'Dharma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_labels(example):\n",
    "    if example['Tag'] in buddhist_labels:\n",
    "        example['Tag'] = 'Buddhist'\n",
    "    else:\n",
    "        example['Tag'] = 'Non-Buddhist'\n",
    "    return example\n",
    "\n",
    "# Apply the function to the dataset\n",
    "ds = ds.map(collapse_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Source', 'Target', 'File_Name', 'Machine Aligned', '__index_level_0__', 'Tag'],\n",
       "        num_rows: 1163105\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Source', 'Target', 'File_Name', 'Machine Aligned', '__index_level_0__', 'Tag'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Labels to Id Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = list(set(ds['train']['Tag']))\n",
    "\n",
    "# Create a label-to-index mapping\n",
    "label2id = {label: idx for idx, label in enumerate(all_tags)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# Save label mappings for future use\n",
    "import json\n",
    "with open(\"bin_op_label_mapping.json\", \"w\") as f:\n",
    "    json.dump(label2id, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Non-Buddhist', 'Buddhist']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    tokens = tokenizer(examples[\"Target\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    tokens[\"labels\"] = [label2id[label] for label in examples[\"Tag\"]]    \n",
    "    return tokens\n",
    "\n",
    "encoded_dataset = ds.map(preprocess, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = encoded_dataset.remove_columns(['Source', 'Target', 'File_Name', 'Machine Aligned', '__index_level_0__', 'Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = encoded_dataset['train'].train_test_split(.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load tokenizer and model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-large-cased\", num_labels=len(label2id))\n",
    "\n",
    "# Resize embeddings to match the new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, references = eval_pred\n",
    "    \n",
    "    # Get predicted class indices\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(references, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(references, predictions, average=\"weighted\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4465/310429104.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbillingsmoore\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/j/Desktop/MLotsawa/Notebooks/Models/BertTag/wandb/run-20250104_235233-mct5ca2f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/billingsmoore/huggingface/runs/mct5ca2f' target=\"_blank\">en-lg-bin-op-bert-classifier</a></strong> to <a href='https://wandb.ai/billingsmoore/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/billingsmoore/huggingface' target=\"_blank\">https://wandb.ai/billingsmoore/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/billingsmoore/huggingface/runs/mct5ca2f' target=\"_blank\">https://wandb.ai/billingsmoore/huggingface/runs/mct5ca2f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324b47d8f31e4b67bf3ac5f5068907d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6179000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3397, 'grad_norm': 7.9321184158325195, 'learning_rate': 1.9998381615148082e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3096, 'grad_norm': 3.769512176513672, 'learning_rate': 1.9996763230296166e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3059, 'grad_norm': 3.2693333625793457, 'learning_rate': 1.9995144845444247e-05, 'epoch': 0.02}\n",
      "{'loss': 0.2897, 'grad_norm': 4.172903537750244, 'learning_rate': 1.999352646059233e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2876, 'grad_norm': 3.770603895187378, 'learning_rate': 1.9991908075740415e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2864, 'grad_norm': 13.307938575744629, 'learning_rate': 1.9990289690888496e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2908, 'grad_norm': 4.294888973236084, 'learning_rate': 1.9988671306036576e-05, 'epoch': 0.06}\n",
      "{'loss': 0.2745, 'grad_norm': 2.7505578994750977, 'learning_rate': 1.998705292118466e-05, 'epoch': 0.06}\n",
      "{'loss': 0.285, 'grad_norm': 4.877462863922119, 'learning_rate': 1.998543453633274e-05, 'epoch': 0.07}\n",
      "{'loss': 0.2738, 'grad_norm': 6.4428629875183105, 'learning_rate': 1.9983816151480825e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2775, 'grad_norm': 4.236544609069824, 'learning_rate': 1.9982197766628905e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2789, 'grad_norm': 6.0026774406433105, 'learning_rate': 1.9980579381776986e-05, 'epoch': 0.1}\n",
      "{'loss': 0.2834, 'grad_norm': 6.753546237945557, 'learning_rate': 1.997896099692507e-05, 'epoch': 0.11}\n",
      "{'loss': 0.2775, 'grad_norm': 3.473449230194092, 'learning_rate': 1.997734261207315e-05, 'epoch': 0.11}\n",
      "{'loss': 0.2813, 'grad_norm': 1.762218713760376, 'learning_rate': 1.9975724227221235e-05, 'epoch': 0.12}\n",
      "{'loss': 0.2758, 'grad_norm': 4.61623477935791, 'learning_rate': 1.997410584236932e-05, 'epoch': 0.13}\n",
      "{'loss': 0.2843, 'grad_norm': 3.168240785598755, 'learning_rate': 1.99724874575174e-05, 'epoch': 0.14}\n",
      "{'loss': 0.2905, 'grad_norm': 5.407663345336914, 'learning_rate': 1.9970869072665483e-05, 'epoch': 0.15}\n",
      "{'loss': 0.2999, 'grad_norm': 6.395883083343506, 'learning_rate': 1.9969250687813564e-05, 'epoch': 0.15}\n",
      "{'loss': 0.298, 'grad_norm': 5.644162178039551, 'learning_rate': 1.9967632302961645e-05, 'epoch': 0.16}\n",
      "{'loss': 0.3002, 'grad_norm': 8.929715156555176, 'learning_rate': 1.996601391810973e-05, 'epoch': 0.17}\n",
      "{'loss': 0.295, 'grad_norm': 20.436290740966797, 'learning_rate': 1.996439553325781e-05, 'epoch': 0.18}\n",
      "{'loss': 0.2944, 'grad_norm': 3.256875991821289, 'learning_rate': 1.9962777148405893e-05, 'epoch': 0.19}\n",
      "{'loss': 0.2985, 'grad_norm': 5.23272705078125, 'learning_rate': 1.9961158763553974e-05, 'epoch': 0.19}\n",
      "{'loss': 0.2947, 'grad_norm': 1.468936800956726, 'learning_rate': 1.9959540378702058e-05, 'epoch': 0.2}\n",
      "{'loss': 0.2929, 'grad_norm': 10.62280559539795, 'learning_rate': 1.995792199385014e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3111, 'grad_norm': 8.445474624633789, 'learning_rate': 1.9956303608998223e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3097, 'grad_norm': 4.800131797790527, 'learning_rate': 1.9954685224146303e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3013, 'grad_norm': 10.257163047790527, 'learning_rate': 1.9953066839294387e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3309, 'grad_norm': 4.117786884307861, 'learning_rate': 1.9951448454442468e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3288, 'grad_norm': 1.445385456085205, 'learning_rate': 1.994983006959055e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3956, 'grad_norm': 7.155774116516113, 'learning_rate': 1.9948211684738633e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3403, 'grad_norm': 2.1596901416778564, 'learning_rate': 1.9946593299886713e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4173, 'grad_norm': 1.3867945671081543, 'learning_rate': 1.9944974915034797e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3712, 'grad_norm': 4.84279727935791, 'learning_rate': 1.994335653018288e-05, 'epoch': 0.28}\n",
      "{'loss': 0.4066, 'grad_norm': 92.59175109863281, 'learning_rate': 1.9941738145330962e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3883, 'grad_norm': 3.9628632068634033, 'learning_rate': 1.9940119760479046e-05, 'epoch': 0.3}\n",
      "{'loss': 0.4018, 'grad_norm': 2.2522647380828857, 'learning_rate': 1.9938501375627126e-05, 'epoch': 0.31}\n",
      "{'loss': 0.4403, 'grad_norm': 3.8938100337982178, 'learning_rate': 1.9936882990775207e-05, 'epoch': 0.32}\n",
      "{'loss': 0.4362, 'grad_norm': 8.598938941955566, 'learning_rate': 1.993526460592329e-05, 'epoch': 0.32}\n",
      "{'loss': 0.5632, 'grad_norm': 4.452981948852539, 'learning_rate': 1.9933646221071372e-05, 'epoch': 0.33}\n",
      "{'loss': 0.5704, 'grad_norm': 2.783557176589966, 'learning_rate': 1.9932027836219456e-05, 'epoch': 0.34}\n",
      "{'loss': 0.587, 'grad_norm': 2.1897740364074707, 'learning_rate': 1.9930409451367536e-05, 'epoch': 0.35}\n",
      "{'loss': 0.5941, 'grad_norm': 3.2258028984069824, 'learning_rate': 1.9928791066515617e-05, 'epoch': 0.36}\n",
      "{'loss': 0.6823, 'grad_norm': 5.205325603485107, 'learning_rate': 1.99271726816637e-05, 'epoch': 0.36}\n",
      "{'loss': 0.6796, 'grad_norm': 3.740926504135132, 'learning_rate': 1.9925554296811785e-05, 'epoch': 0.37}\n",
      "{'loss': 0.6759, 'grad_norm': 7.675320148468018, 'learning_rate': 1.9923935911959866e-05, 'epoch': 0.38}\n",
      "{'loss': 0.6761, 'grad_norm': 1.6830239295959473, 'learning_rate': 1.992231752710795e-05, 'epoch': 0.39}\n",
      "{'loss': 0.6756, 'grad_norm': 4.56446647644043, 'learning_rate': 1.992069914225603e-05, 'epoch': 0.4}\n",
      "{'loss': 0.6772, 'grad_norm': 1.7266546487808228, 'learning_rate': 1.991908075740411e-05, 'epoch': 0.4}\n",
      "{'loss': 0.6744, 'grad_norm': 3.432800531387329, 'learning_rate': 1.9917462372552195e-05, 'epoch': 0.41}\n",
      "{'loss': 0.6785, 'grad_norm': 1.9271292686462402, 'learning_rate': 1.9915843987700276e-05, 'epoch': 0.42}\n",
      "{'loss': 0.6474, 'grad_norm': 3.3614025115966797, 'learning_rate': 1.991422560284836e-05, 'epoch': 0.43}\n",
      "{'loss': 0.6581, 'grad_norm': 1.558332085609436, 'learning_rate': 1.991260721799644e-05, 'epoch': 0.44}\n",
      "{'loss': 0.6737, 'grad_norm': 5.613613605499268, 'learning_rate': 1.9910988833144524e-05, 'epoch': 0.45}\n",
      "{'loss': 0.6724, 'grad_norm': 3.917166233062744, 'learning_rate': 1.9909370448292605e-05, 'epoch': 0.45}\n",
      "{'loss': 0.6716, 'grad_norm': 3.91076397895813, 'learning_rate': 1.990775206344069e-05, 'epoch': 0.46}\n",
      "{'loss': 0.6708, 'grad_norm': 3.8543384075164795, 'learning_rate': 1.990613367858877e-05, 'epoch': 0.47}\n",
      "{'loss': 0.6712, 'grad_norm': 2.581003427505493, 'learning_rate': 1.9904515293736854e-05, 'epoch': 0.48}\n",
      "{'loss': 0.6681, 'grad_norm': 6.953207492828369, 'learning_rate': 1.9902896908884934e-05, 'epoch': 0.49}\n",
      "{'loss': 0.671, 'grad_norm': 3.6506383419036865, 'learning_rate': 1.9901278524033015e-05, 'epoch': 0.49}\n",
      "{'loss': 0.6737, 'grad_norm': 4.958682060241699, 'learning_rate': 1.98996601391811e-05, 'epoch': 0.5}\n",
      "{'loss': 0.6643, 'grad_norm': 2.3508613109588623, 'learning_rate': 1.989804175432918e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6706, 'grad_norm': 2.6127867698669434, 'learning_rate': 1.9896423369477263e-05, 'epoch': 0.52}\n",
      "{'loss': 0.673, 'grad_norm': 2.8800084590911865, 'learning_rate': 1.9894804984625347e-05, 'epoch': 0.53}\n",
      "{'loss': 0.669, 'grad_norm': 2.816453456878662, 'learning_rate': 1.9893186599773428e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6736, 'grad_norm': 6.620913028717041, 'learning_rate': 1.9891568214921512e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6729, 'grad_norm': 6.050339698791504, 'learning_rate': 1.9889949830069593e-05, 'epoch': 0.55}\n",
      "{'loss': 0.6706, 'grad_norm': 6.074922561645508, 'learning_rate': 1.9888331445217673e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6683, 'grad_norm': 4.77290678024292, 'learning_rate': 1.9886713060365757e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6655, 'grad_norm': 3.450300931930542, 'learning_rate': 1.9885094675513838e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6649, 'grad_norm': 2.639470338821411, 'learning_rate': 1.9883476290661922e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6726, 'grad_norm': 1.5344609022140503, 'learning_rate': 1.9881857905810003e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6655, 'grad_norm': 4.934468746185303, 'learning_rate': 1.9880239520958083e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6716, 'grad_norm': 2.8312618732452393, 'learning_rate': 1.9878621136106167e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6682, 'grad_norm': 6.068465709686279, 'learning_rate': 1.987700275125425e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6685, 'grad_norm': 3.56724214553833, 'learning_rate': 1.9875384366402332e-05, 'epoch': 0.62}\n",
      "{'loss': 0.6697, 'grad_norm': 3.1684393882751465, 'learning_rate': 1.9873765981550416e-05, 'epoch': 0.63}\n",
      "{'loss': 0.671, 'grad_norm': 1.559934377670288, 'learning_rate': 1.9872147596698497e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6647, 'grad_norm': 7.126791000366211, 'learning_rate': 1.9870529211846577e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6674, 'grad_norm': 1.5278929471969604, 'learning_rate': 1.986891082699466e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6678, 'grad_norm': 4.579922199249268, 'learning_rate': 1.9867292442142742e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6628, 'grad_norm': 3.3197121620178223, 'learning_rate': 1.9865674057290826e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6688, 'grad_norm': 6.077704906463623, 'learning_rate': 1.9864055672438906e-05, 'epoch': 0.68}\n",
      "{'loss': 0.6697, 'grad_norm': 1.9336683750152588, 'learning_rate': 1.986243728758699e-05, 'epoch': 0.69}\n",
      "{'loss': 0.6665, 'grad_norm': 2.1877658367156982, 'learning_rate': 1.9860818902735074e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6613, 'grad_norm': 3.591625213623047, 'learning_rate': 1.9859200517883155e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6639, 'grad_norm': 1.8716750144958496, 'learning_rate': 1.9857582133031236e-05, 'epoch': 0.71}\n",
      "{'loss': 0.6684, 'grad_norm': 6.3957672119140625, 'learning_rate': 1.985596374817932e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6642, 'grad_norm': 1.6225155591964722, 'learning_rate': 1.98543453633274e-05, 'epoch': 0.73}\n",
      "{'loss': 0.665, 'grad_norm': 5.877086639404297, 'learning_rate': 1.9852726978475484e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6643, 'grad_norm': 3.9500925540924072, 'learning_rate': 1.9851108593623565e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6695, 'grad_norm': 2.5310065746307373, 'learning_rate': 1.9849490208771646e-05, 'epoch': 0.75}\n",
      "{'loss': 0.667, 'grad_norm': 2.9335882663726807, 'learning_rate': 1.984787182391973e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6662, 'grad_norm': 1.3791238069534302, 'learning_rate': 1.9846253439067814e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6711, 'grad_norm': 4.316354274749756, 'learning_rate': 1.9844635054215894e-05, 'epoch': 0.78}\n",
      "{'loss': 0.6629, 'grad_norm': 2.688131093978882, 'learning_rate': 1.984301666936398e-05, 'epoch': 0.78}\n",
      "{'loss': 0.6636, 'grad_norm': 1.4180151224136353, 'learning_rate': 1.984139828451206e-05, 'epoch': 0.79}\n",
      "{'loss': 0.6618, 'grad_norm': 1.3589518070220947, 'learning_rate': 1.983977989966014e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6652, 'grad_norm': 5.307155132293701, 'learning_rate': 1.9838161514808224e-05, 'epoch': 0.81}\n",
      "{'loss': 0.6706, 'grad_norm': 2.9058210849761963, 'learning_rate': 1.9836543129956304e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6671, 'grad_norm': 4.911036968231201, 'learning_rate': 1.9834924745104388e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6667, 'grad_norm': 4.357964992523193, 'learning_rate': 1.983330636025247e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6652, 'grad_norm': 5.278253078460693, 'learning_rate': 1.983168797540055e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6649, 'grad_norm': 2.064035177230835, 'learning_rate': 1.9830069590548634e-05, 'epoch': 0.85}\n",
      "{'loss': 0.6639, 'grad_norm': 2.2009544372558594, 'learning_rate': 1.9828451205696718e-05, 'epoch': 0.86}\n",
      "{'loss': 0.661, 'grad_norm': 1.846257209777832, 'learning_rate': 1.9826832820844798e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6674, 'grad_norm': 3.7096505165100098, 'learning_rate': 1.9825214435992882e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6621, 'grad_norm': 2.364447593688965, 'learning_rate': 1.9823596051140963e-05, 'epoch': 0.88}\n",
      "{'loss': 0.664, 'grad_norm': 2.2678489685058594, 'learning_rate': 1.9821977666289043e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6642, 'grad_norm': 1.8881648778915405, 'learning_rate': 1.9820359281437127e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6636, 'grad_norm': 2.7891504764556885, 'learning_rate': 1.9818740896585208e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6657, 'grad_norm': 3.4515652656555176, 'learning_rate': 1.9817122511733292e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6664, 'grad_norm': 3.937854051589966, 'learning_rate': 1.9815504126881373e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6618, 'grad_norm': 4.231456279754639, 'learning_rate': 1.9813885742029457e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6684, 'grad_norm': 5.30842399597168, 'learning_rate': 1.981226735717754e-05, 'epoch': 0.94}\n",
      "{'loss': 0.6695, 'grad_norm': 2.311948776245117, 'learning_rate': 1.981064897232562e-05, 'epoch': 0.95}\n",
      "{'loss': 0.663, 'grad_norm': 1.801082730293274, 'learning_rate': 1.9809030587473702e-05, 'epoch': 0.95}\n",
      "{'loss': 0.6689, 'grad_norm': 5.527515888214111, 'learning_rate': 1.9807412202621786e-05, 'epoch': 0.96}\n",
      "{'loss': 0.6632, 'grad_norm': 2.096284866333008, 'learning_rate': 1.9805793817769867e-05, 'epoch': 0.97}\n",
      "{'loss': 0.6628, 'grad_norm': 6.073912143707275, 'learning_rate': 1.980417543291795e-05, 'epoch': 0.98}\n",
      "{'loss': 0.6634, 'grad_norm': 1.26802396774292, 'learning_rate': 1.980255704806603e-05, 'epoch': 0.99}\n",
      "{'loss': 0.6676, 'grad_norm': 1.5050489902496338, 'learning_rate': 1.9800938663214112e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091a2a7d90ae489c90ef80d4aa680498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10905 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Desktop/MLotsawa/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7154206037521362, 'eval_accuracy': 0.38903282014833834, 'eval_f1': 0.2179164278298431, 'eval_precision': 0.15134653515256938, 'eval_recall': 0.38903282014833834, 'eval_runtime': 1184.4032, 'eval_samples_per_second': 147.303, 'eval_steps_per_second': 9.207, 'epoch': 1.0}\n",
      "{'loss': 0.6612, 'grad_norm': 1.3529378175735474, 'learning_rate': 1.9799320278362196e-05, 'epoch': 1.0}\n",
      "{'loss': 0.6672, 'grad_norm': 2.4305572509765625, 'learning_rate': 1.979770189351028e-05, 'epoch': 1.01}\n",
      "{'loss': 0.6642, 'grad_norm': 4.355087757110596, 'learning_rate': 1.979608350865836e-05, 'epoch': 1.02}\n",
      "{'loss': 0.6629, 'grad_norm': 1.5011831521987915, 'learning_rate': 1.9794465123806445e-05, 'epoch': 1.03}\n",
      "{'loss': 0.6661, 'grad_norm': 1.5247712135314941, 'learning_rate': 1.9792846738954525e-05, 'epoch': 1.04}\n",
      "{'loss': 0.6616, 'grad_norm': 1.2828189134597778, 'learning_rate': 1.9791228354102606e-05, 'epoch': 1.04}\n",
      "{'loss': 0.6654, 'grad_norm': 1.2788029909133911, 'learning_rate': 1.978960996925069e-05, 'epoch': 1.05}\n",
      "{'loss': 0.6628, 'grad_norm': 5.623254299163818, 'learning_rate': 1.978799158439877e-05, 'epoch': 1.06}\n",
      "{'loss': 0.665, 'grad_norm': 2.610895872116089, 'learning_rate': 1.9786373199546854e-05, 'epoch': 1.07}\n",
      "{'loss': 0.6554, 'grad_norm': 3.9459547996520996, 'learning_rate': 1.9784754814694935e-05, 'epoch': 1.08}\n",
      "{'loss': 0.661, 'grad_norm': 1.581750512123108, 'learning_rate': 1.9783136429843016e-05, 'epoch': 1.08}\n",
      "{'loss': 0.6601, 'grad_norm': 1.5955774784088135, 'learning_rate': 1.9781518044991103e-05, 'epoch': 1.09}\n",
      "{'loss': 0.6618, 'grad_norm': 3.2298269271850586, 'learning_rate': 1.9779899660139184e-05, 'epoch': 1.1}\n",
      "{'loss': 0.6632, 'grad_norm': 2.3392815589904785, 'learning_rate': 1.9778281275287264e-05, 'epoch': 1.11}\n",
      "{'loss': 0.6684, 'grad_norm': 1.4654797315597534, 'learning_rate': 1.977666289043535e-05, 'epoch': 1.12}\n",
      "{'loss': 0.6647, 'grad_norm': 1.4004204273223877, 'learning_rate': 1.977504450558343e-05, 'epoch': 1.12}\n",
      "{'loss': 0.6645, 'grad_norm': 1.610036015510559, 'learning_rate': 1.9773426120731513e-05, 'epoch': 1.13}\n",
      "{'loss': 0.6611, 'grad_norm': 5.37900447845459, 'learning_rate': 1.9771807735879594e-05, 'epoch': 1.14}\n",
      "{'loss': 0.6595, 'grad_norm': 1.340243935585022, 'learning_rate': 1.9770189351027674e-05, 'epoch': 1.15}\n",
      "{'loss': 0.6634, 'grad_norm': 3.0651755332946777, 'learning_rate': 1.976857096617576e-05, 'epoch': 1.16}\n",
      "{'loss': 0.6594, 'grad_norm': 1.0808420181274414, 'learning_rate': 1.976695258132384e-05, 'epoch': 1.17}\n",
      "{'loss': 0.6657, 'grad_norm': 4.829565048217773, 'learning_rate': 1.9765334196471923e-05, 'epoch': 1.17}\n",
      "{'loss': 0.6554, 'grad_norm': 1.2802166938781738, 'learning_rate': 1.9763715811620007e-05, 'epoch': 1.18}\n",
      "{'loss': 0.6593, 'grad_norm': 3.6355581283569336, 'learning_rate': 1.9762097426768088e-05, 'epoch': 1.19}\n",
      "{'loss': 0.6645, 'grad_norm': 1.3910729885101318, 'learning_rate': 1.9760479041916168e-05, 'epoch': 1.2}\n",
      "{'loss': 0.6566, 'grad_norm': 3.3977015018463135, 'learning_rate': 1.9758860657064252e-05, 'epoch': 1.21}\n",
      "{'loss': 0.6607, 'grad_norm': 2.3093488216400146, 'learning_rate': 1.9757242272212333e-05, 'epoch': 1.21}\n",
      "{'loss': 0.6626, 'grad_norm': 3.345737934112549, 'learning_rate': 1.9755623887360417e-05, 'epoch': 1.22}\n",
      "{'loss': 0.6652, 'grad_norm': 3.2134037017822266, 'learning_rate': 1.9754005502508498e-05, 'epoch': 1.23}\n",
      "{'loss': 0.6581, 'grad_norm': 2.0353543758392334, 'learning_rate': 1.9752387117656578e-05, 'epoch': 1.24}\n",
      "{'loss': 0.6618, 'grad_norm': 1.2238274812698364, 'learning_rate': 1.9750768732804662e-05, 'epoch': 1.25}\n",
      "{'loss': 0.6618, 'grad_norm': 1.2283389568328857, 'learning_rate': 1.9749150347952746e-05, 'epoch': 1.25}\n",
      "{'loss': 0.6603, 'grad_norm': 1.2488458156585693, 'learning_rate': 1.9747531963100827e-05, 'epoch': 1.26}\n",
      "{'loss': 0.6589, 'grad_norm': 2.2565858364105225, 'learning_rate': 1.974591357824891e-05, 'epoch': 1.27}\n",
      "{'loss': 0.6582, 'grad_norm': 1.303315281867981, 'learning_rate': 1.974429519339699e-05, 'epoch': 1.28}\n",
      "{'loss': 0.6597, 'grad_norm': 3.227710008621216, 'learning_rate': 1.9742676808545072e-05, 'epoch': 1.29}\n",
      "{'loss': 0.6612, 'grad_norm': 3.7910258769989014, 'learning_rate': 1.9741058423693156e-05, 'epoch': 1.29}\n",
      "{'loss': 0.657, 'grad_norm': 1.8623722791671753, 'learning_rate': 1.9739440038841237e-05, 'epoch': 1.3}\n",
      "{'loss': 0.6627, 'grad_norm': 3.8118977546691895, 'learning_rate': 1.973782165398932e-05, 'epoch': 1.31}\n",
      "{'loss': 0.6668, 'grad_norm': 1.3592146635055542, 'learning_rate': 1.97362032691374e-05, 'epoch': 1.32}\n",
      "{'loss': 0.6612, 'grad_norm': 2.5002152919769287, 'learning_rate': 1.9734584884285482e-05, 'epoch': 1.33}\n",
      "{'loss': 0.6642, 'grad_norm': 4.604518890380859, 'learning_rate': 1.973296649943357e-05, 'epoch': 1.34}\n",
      "{'loss': 0.6581, 'grad_norm': 4.797727108001709, 'learning_rate': 1.973134811458165e-05, 'epoch': 1.34}\n",
      "{'loss': 0.6547, 'grad_norm': 1.6001827716827393, 'learning_rate': 1.972972972972973e-05, 'epoch': 1.35}\n",
      "{'loss': 0.6574, 'grad_norm': 3.751911163330078, 'learning_rate': 1.9728111344877815e-05, 'epoch': 1.36}\n",
      "{'loss': 0.6503, 'grad_norm': 3.717975616455078, 'learning_rate': 1.9726492960025895e-05, 'epoch': 1.37}\n",
      "{'loss': 0.6596, 'grad_norm': 2.537698984146118, 'learning_rate': 1.972487457517398e-05, 'epoch': 1.38}\n",
      "{'loss': 0.6669, 'grad_norm': 2.2880890369415283, 'learning_rate': 1.972325619032206e-05, 'epoch': 1.38}\n",
      "{'loss': 0.6615, 'grad_norm': 3.57985782623291, 'learning_rate': 1.972163780547014e-05, 'epoch': 1.39}\n",
      "{'loss': 0.6622, 'grad_norm': 4.158631324768066, 'learning_rate': 1.9720019420618225e-05, 'epoch': 1.4}\n",
      "{'loss': 0.6599, 'grad_norm': 2.179264545440674, 'learning_rate': 1.9718401035766305e-05, 'epoch': 1.41}\n",
      "{'loss': 0.6593, 'grad_norm': 4.131139755249023, 'learning_rate': 1.971678265091439e-05, 'epoch': 1.42}\n",
      "{'loss': 0.66, 'grad_norm': 3.277925968170166, 'learning_rate': 1.9715164266062473e-05, 'epoch': 1.42}\n",
      "{'loss': 0.6647, 'grad_norm': 1.5723211765289307, 'learning_rate': 1.9713545881210554e-05, 'epoch': 1.43}\n",
      "{'loss': 0.6609, 'grad_norm': 1.4110119342803955, 'learning_rate': 1.9711927496358634e-05, 'epoch': 1.44}\n",
      "{'loss': 0.661, 'grad_norm': 1.5413293838500977, 'learning_rate': 1.971030911150672e-05, 'epoch': 1.45}\n",
      "{'loss': 0.6618, 'grad_norm': 2.6766257286071777, 'learning_rate': 1.97086907266548e-05, 'epoch': 1.46}\n",
      "{'loss': 0.6673, 'grad_norm': 2.727954864501953, 'learning_rate': 1.9707072341802883e-05, 'epoch': 1.46}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 36\u001b[0m\n\u001b[1;32m     25\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     26\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     27\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping]  \u001b[38;5;66;03m# Add the early stopping callback\u001b[39;00m\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/MLotsawa/.venv/lib/python3.12/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/MLotsawa/.venv/lib/python3.12/site-packages/transformers/trainer.py:2527\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2522\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2525\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2528\u001b[0m ):\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2530\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"en-lg-bin-op-bert-classifier\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=100,  # Set a maximum number of epochs\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",  # Evaluate at the end of every epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of every epoch\n",
    "    load_best_model_at_end=True,  # Load the best model after training\n",
    "    metric_for_best_model=\"accuracy\",  # Metric to monitor\n",
    "    greater_is_better=True,  # Higher accuracy is better\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# Add the EarlyStoppingCallback\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3  # Stop training if the metric does not improve for 3 evaluation steps\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping]  # Add the early stopping callback\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
