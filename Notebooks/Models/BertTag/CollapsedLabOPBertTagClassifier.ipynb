{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('openpecha/tagged_cleaned_MT_v1.0.3')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('tibetan_tokenizer')\n",
    "\n",
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Blank Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition(example):\n",
    "    return example['Tag'] != ''\n",
    "\n",
    "ds = ds.filter(condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collapse Buddhist Lables into One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buddhist_labels = ['Mantras',\n",
    "                    'Dzogchen',\n",
    "                    'Astrology',\n",
    "                    'Monastery',\n",
    "                    'Mahamudra',\n",
    "                    'Mind',\n",
    "                    'Meditation',\n",
    "                    'Self, Logic, Aggregates',\n",
    "                    'Tantra',\n",
    "                    'Emptiness',\n",
    "                    'Dreams',\n",
    "                    'Education, Teaching',\n",
    "                    'Ethics, Enlightenment, Wisdom',\n",
    "                    'Prophecies, Rituals',\n",
    "                    'Lama',\n",
    "                    'Samsara, Nirvana',\n",
    "                    'Milarepa, Realization, Biography',\n",
    "                    'Kayas',\n",
    "                    'Intrinsic Existence, Conventional Existence',\n",
    "                    'Time, Causality, Perception',\n",
    "                    'Natural State',\n",
    "                    'Karma, Consequences',\n",
    "                    'Dharma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_labels(example):\n",
    "    if example['Tag'] in buddhist_labels:\n",
    "        example['Tag'] = 'Buddhist'\n",
    "    return example\n",
    "\n",
    "# Apply the function to the dataset\n",
    "ds = ds.map(collapse_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Labels to Id Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = list(set(ds['train']['Tag']))\n",
    "\n",
    "# Create a label-to-index mapping\n",
    "label2id = {label: idx for idx, label in enumerate(all_tags)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# Save label mappings for future use\n",
    "import json\n",
    "with open(\"simple_op_label_mapping.json\", \"w\") as f:\n",
    "    json.dump(label2id, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    tokens = tokenizer(examples[\"Source\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    tokens[\"labels\"] = [label2id[label] for label in examples[\"Tag\"]]    \n",
    "    return tokens\n",
    "\n",
    "encoded_dataset = ds.map(preprocess, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = encoded_dataset.remove_columns(['Source', 'Target', 'File_Name', 'Machine Aligned', '__index_level_0__', 'Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = encoded_dataset['train'].train_test_split(.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load tokenizer and model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label2id))\n",
    "\n",
    "# Resize embeddings to match the new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, references = eval_pred\n",
    "    \n",
    "    # Get predicted class indices\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(references, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(references, predictions, average=\"weighted\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"fixed-col-op-bert-classifier\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=100,  # Set a maximum number of epochs\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",  # Evaluate at the end of every epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of every epoch\n",
    "    load_best_model_at_end=True,  # Load the best model after training\n",
    "    metric_for_best_model=\"accuracy\",  # Metric to monitor\n",
    "    greater_is_better=True,  # Higher accuracy is better\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# Add the EarlyStoppingCallback\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3  # Stop training if the metric does not improve for 3 evaluation steps\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping]  # Add the early stopping callback\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
