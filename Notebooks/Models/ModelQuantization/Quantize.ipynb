{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Quantization\n",
    "\n",
    "This notebook documents the process of quantizing a translation model and can be used as a template for all quantizations.\n",
    "\n",
    "## Load Tokenizer and Model With Quantization Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"billingsmoore/tibetan-to-english-translation\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "quantization_config =  BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check That Model Has A Reduced Memory Footprint\n",
    "\n",
    "The size of the original model can be seen by looking at the file size in the relevant Hugging Face repo. The line of code below prints the memory footprint of the quantized model measured in bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1123254272"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Model to Hub\n",
    "\n",
    "The line of code below pushes the model to the Hugging Face Hub. You may need to use notebook_login() to log in to Hugging Face and provide an API key.\n",
    "\n",
    "Ensure that the quantized model has an appropriate model id. Ideally it should be the name of the model that is quantizes with the degree of quantization appended, i.e. 'tibetan-to-english-translation' is being quantized to 4bit and thus becomes 'tibetan-to-english-translation-4bit'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97b1483da3946c2a15dede5cec78ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/billingsmoore/tibetan-to-english-translation-4bit/commit/ff96bc443f0801bf8f00fcab8503fa9993f802e9', commit_message='Upload T5ForConditionalGeneration', commit_description='', oid='ff96bc443f0801bf8f00fcab8503fa9993f802e9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('billingsmoore/tibetan-to-english-translation-4bit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Out Model Card\n",
    "\n",
    "Once the model has been successfully pushed to the Hub, make sure to fill out the model card and provide the quantization configuration settings there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
