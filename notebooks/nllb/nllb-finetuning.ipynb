{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning NLLB\n",
    "\n",
    "The purpose of this notebook is document the process of fine-tuning an NLLB model for translating from Literary Tibetan to English. \n",
    "\n",
    "Some of the code in this notebook is based on the the tutorial ['How To Fine Tune a NLLB 200 Model for Translating A New Language'](https://cointegrated.medium.com/how-to-fine-tune-a-nllb-200-model-for-translating-a-new-language-a37fc706b865). However, the training loop and preprocessing have been heavily revised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "from transformers.optimization import Adafactor\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import trange\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text Pairs\n",
    "\n",
    "### Loading the Data\n",
    "\n",
    "The code below loads in the text pairs as a list, [Tibetan, English]. Then batches them. This is a helper function for the custom training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_pairs(data, batch_size, num_batches):\n",
    "\n",
    "    print(f'Loading {data}...', end='\\r')\n",
    "    \n",
    "    data_path = '../../data/training-batches/' + data\n",
    "\n",
    "    with open(data_path) as f:\n",
    "        lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "    pairs = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            tib, eng = line.split(\",\")[:2]\n",
    "            eng = eng.lower()\n",
    "            pairs.append([tib, eng])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(f'Batching {data}... ', end='\\r')\n",
    "    \n",
    "    copy = pairs.copy()\n",
    "    batches = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        xx, yy = [], []\n",
    "        for _ in range(batch_size):\n",
    "            i = random.randint(0, len(copy)-1)\n",
    "            item = copy[i]\n",
    "            xx.append(item[0])\n",
    "            yy.append(item[1])\n",
    "            del copy[i]\n",
    "        batches.append([xx, yy])\n",
    "\n",
    "    print(f'Training on {data}             ')\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "### Pre-Trained Model\n",
    "Here, I've downloaded the pre-trained NLLB model and its associated tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/j/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\").cuda()\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/home/j/Documents/Projects/MLotsawa/notebooks/nllb/nllb-checkpoint-0\").cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Below, I've selected the Adafactor optimizer for training. The values passed to the optimizer are taken from the tutorial mentioned above and are arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adafactor(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    scale_parameter=False,\n",
    "    relative_step=False,\n",
    "    lr=1e-4,\n",
    "    clip_threshold=1.0,\n",
    "    weight_decay=1e-3,\n",
    ")\n",
    "scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = 'nllb-checkpoint-0.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "Below, I've written a custom training. The first draft was adapted from the previously mentioned tutorial but it has since been substantially re-written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = []\n",
    "\n",
    "epoch_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_dir, optimizer, batch_size=16, epochs=1):\n",
    "    global all_losses, epoch_losses\n",
    "    \n",
    "    x, y, loss = None, None, None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    steps_per_batch = 100\n",
    "\n",
    "    \n",
    "\n",
    "    for _ in range(epochs):\n",
    "\n",
    "            remaining_shards = os.listdir(data_dir)\n",
    "\n",
    "            losses = []  # simple tracking of average loss\n",
    "\n",
    "            for i in range(len(os.listdir(data_dir))):\n",
    "\n",
    "                random.shuffle(remaining_shards)\n",
    "\n",
    "                shard = remaining_shards[0]\n",
    "\n",
    "                del remaining_shards[0]\n",
    "\n",
    "\n",
    "                batches = get_batch_pairs(shard, batch_size, steps_per_batch)\n",
    "\n",
    "                shard_losses = []\n",
    "\n",
    "                desc = ('Epoch '+str(_)+', Shard: ' + str(shard))\n",
    "                tq = trange(len(shard_losses), steps_per_batch, desc=desc) # take 100 random batches from each data shard\n",
    "                \n",
    "                for i in tq:\n",
    "\n",
    "                    xx, yy = batches[i][0], batches[i][1]\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        tokenizer.src_lang = 'bo'\n",
    "                        x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=128).to(model.device)\n",
    "                        tokenizer.src_lang = 'eng_Latn'\n",
    "                        y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=128).to(model.device)\n",
    "                        # -100 is a magic value ignored in the loss function\n",
    "                        # because we don't want the model to learn to predict padding ids\n",
    "                        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n",
    "\n",
    "                        loss = model(**x, labels=y.input_ids).loss\n",
    "                        loss.backward()\n",
    "                        losses.append(loss.item())\n",
    "                        shard_losses.append(loss.item())\n",
    "\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad(set_to_none=True)\n",
    "                        scheduler.step()\n",
    "\n",
    "                        print('loss: ' + str(np.mean(losses)),  end=\"\\r\")\n",
    "\n",
    "                    except RuntimeError as e:  # usually, it is out-of-memory\n",
    "                        optimizer.zero_grad(set_to_none=True)\n",
    "                        x, y, loss = None, None, None\n",
    "                        gc.collect()\n",
    "                        torch.cuda.empty_cache()\n",
    "                        continue\n",
    "\n",
    "                model.save_pretrained(MODEL_SAVE_PATH+str(_))\n",
    "                print('loss: ' + str(np.mean(losses)))\n",
    "                all_losses+=losses\n",
    "                try:\n",
    "                    plt.close()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                ys1 = all_losses\n",
    "                xs = [x for x in range(len(ys1))]\n",
    "\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(xs, ys1)\n",
    "\n",
    "                ys2 = epoch_losses\n",
    "                xs = [x for x in range(len(ys2))]\n",
    "\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(xs, ys2)\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "            epoch_losses.append(losses[-1])\n",
    "\n",
    "    return [all_losses, epoch_losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on training-batch-4.txt             \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40761f92d2b8486a972556b364cd52ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0, Shard: training-batch-4.txt:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.16912786450237036\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/training-batches\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 66\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data_dir, optimizer, batch_size, epochs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_SAVE_PATH\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(losses)))\n\u001b[1;32m     68\u001b[0m all_losses\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mlosses\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2114\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2112\u001b[0m         safe_save_file(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file), metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m   2113\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2114\u001b[0m         \u001b[43msave_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2117\u001b[0m     path_to_weights \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, _add_variant(WEIGHTS_NAME, variant))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:629\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 629\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:863\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    862\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 863\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = train(data_dir='../../data/training-batches', optimizer=optimizer, batch_size=32, epochs=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
