{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 16:11:54.990054: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-09 16:11:54.999812: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-09 16:11:55.012283: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-09 16:11:55.015784: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-09 16:11:55.025075: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-09 16:11:55.726209: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"རང་གི་སྤྱི་བོར་ཆུ་སྐྱེས་གེ་སར་ལྟེར། །\n",
    "ཐབས་ཤེས་ཉི་ཟླ་མཛེས་པའི་ཁྲི་སྟེང་ནས། །\n",
    "དུས་གསུམ་རྒྱལ་ཀུན་ཡེ་ཤེས་འདུས་པའི་གཟུགས། །\n",
    "དཀོན་མཆོག་ཀུན་འདུས་བླ་མ་ཀརྨ་པ། །\n",
    "ཞི་འཛུམ་བདེ་ཆེན་རྒྱས་པའི་ཉམས་བརྒྱ་འབར། །\n",
    "\"\"\"\n",
    "\n",
    "text = text.split(' །')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'billingsmoore/tibetan-phonetic-transliteration'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline('translation',model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'rang gi chiwor chukyé gesar ter'},\n",
       " {'translation_text': 'tabshé nyida dzepé tri teng né'},\n",
       " {'translation_text': 'dü sum gyal kün yeshe düpé zuk'},\n",
       " {'translation_text': 'könchok kündü lama karma pa'},\n",
       " {'translation_text': 'zhi dzum dechen gyepé nyam gya bar'},\n",
       " {'translation_text': ''}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = model = AutoModelForSeq2SeqLM.from_pretrained('google-t5/t5-small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁', 'བོད་ཀྱི་བོད་ཡིག']\n",
      "Encoding (token ids): [3, 2, 1]\n",
      "Decoded text: \n",
      "Mismatch detected: The tokenizer may not be handling Tibetan characters correctly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n",
    "\n",
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Example Tibetan text\n",
    "tibetan_text = \"བོད་ཀྱི་བོད་ཡིག\"\n",
    "\n",
    "# Function to check tokenization\n",
    "def test_tokenization(text):\n",
    "    # Tokenize the Tibetan text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(\"Tokens:\", tokens)\n",
    "\n",
    "    # Encode the Tibetan text\n",
    "    encoding = tokenizer.encode(text)\n",
    "    print(\"Encoding (token ids):\", encoding)\n",
    "\n",
    "    # Decode to verify round-trip accuracy\n",
    "    decoded_text = tokenizer.decode(encoding, skip_special_tokens=True)\n",
    "    print(\"Decoded text:\", decoded_text)\n",
    "\n",
    "    # Check if the original and decoded text match\n",
    "    if text == decoded_text:\n",
    "        print(\"The tokenizer correctly handles Tibetan characters.\")\n",
    "    else:\n",
    "        print(\"Mismatch detected: The tokenizer may not be handling Tibetan characters correctly.\")\n",
    "\n",
    "# Run the test\n",
    "test_tokenization(tibetan_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tokenizer('This is an input', return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gen = model.generate(input_ids=tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dieser input'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(gen[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting botok\n",
      "  Downloading botok-0.8.12-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: pyyaml in /home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages (from botok) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages (from botok) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages (from requests->botok) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages (from requests->botok) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages (from requests->botok) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages (from requests->botok) (2024.7.4)\n",
      "Downloading botok-0.8.12-py3-none-any.whl (79 kB)\n",
      "Installing collected packages: botok\n",
      "Successfully installed botok-0.8.12\n"
     ]
    }
   ],
   "source": [
    "! pip install botok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading general dialect pack ...\n",
      "[INFO] Download completed!\n",
      "Building Trie:\n",
      "\t/home/j/Documents/pybo/dialect_packs/general/dictionary/words_non_inflected/particles.tsv\n",
      "\t/home/j/Documents/pybo/dialect_packs/general/dictionary/words/ancient.tsv\n",
      "\t/home/j/Documents/pybo/dialect_packs/general/dictionary/words/uncompound_lexicon.tsv\n",
      "\t/home/j/Documents/pybo/dialect_packs/general/dictionary/words/tsikchen.tsv\n",
      "\t/home/j/Documents/pybo/dialect_packs/general/dictionary/words/exceptions.tsv\n",
      "\t/home/j/Documents/pybo/dialect_packs/general/dictionary/words/dagdra.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/botok/textunits/bostring.py:82: UserWarning: Beware of unexpected results: input string contains the non-expanded char \"དྷ\", found in \"\".\n",
      "  warn(\n",
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/botok/textunits/bostring.py:82: UserWarning: Beware of unexpected results: input string contains the non-expanded char \"ྡྷ\", found in \"ྡྷ་པཱ་\".\n",
      "  warn(\n",
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/botok/textunits/bostring.py:82: UserWarning: Beware of unexpected results: input string contains the non-expanded char \"བྷ\", found in \"\".\n",
      "  warn(\n",
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/botok/textunits/bostring.py:82: UserWarning: Beware of unexpected results: input string contains the non-expanded char \"ྦྷ\", found in \"\".\n",
      "  warn(\n",
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/botok/textunits/bostring.py:82: UserWarning: Beware of unexpected results: input string contains the non-expanded char \"དྷ\", found in \"དྷྱཱ་\".\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5 s.)\n"
     ]
    }
   ],
   "source": [
    "tok = WordTokenizer().tokenize('བཀྲ་ཤིས་བདེ་ལེགས་ཞུས་རྒྱུ་ཡིན་ སེམས་པ་སྐྱིད་པོ་འདུག།')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: \"སེམས་པ་\"\n",
       "text_cleaned: \"སེམས་པ་\"\n",
       "text_unaffixed: \"སེམས་པ་\"\n",
       "syls: [\"སེམས\", \"པ\"]\n",
       "pos: VERB\n",
       "lemma: སེམས་པ་\n",
       "senses: | pos: VERB, freq: 6274, affixed: False, lemma: སེམས་པ་ |\n",
       "char_types: |CONS|VOW|CONS|CONS|TSEK|CONS|TSEK|\n",
       "chunk_type: TEXT\n",
       "freq: 6274\n",
       "syls_idx: [[0, 1, 2, 3], [5]]\n",
       "syls_start_end: [{'start': 0, 'end': 5}, {'start': 5, 'end': 7}]\n",
       "start: 31\n",
       "len: 7\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading general dialect pack ...\n",
      "[INFO] Download completed!\n",
      "Building Trie:\n",
      "\t/home/j/general/dictionary/words_non_inflected/particles.tsv\n",
      "\t/home/j/general/dictionary/words/ancient.tsv\n",
      "\t/home/j/general/dictionary/words/uncompound_lexicon.tsv\n",
      "\t/home/j/general/dictionary/words/tsikchen.tsv\n",
      "\t/home/j/general/dictionary/words/exceptions.tsv\n",
      "\t/home/j/general/dictionary/words/dagdra.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/botok/textunits/bostring.py:82: UserWarning: Beware of unexpected results: input string contains the non-expanded char \"དྷ\", found in \"\".\n",
      "  warn(\n",
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/botok/textunits/bostring.py:82: UserWarning: Beware of unexpected results: input string contains the non-expanded char \"ྡྷ\", found in \"ྡྷ་པཱ་\".\n",
      "  warn(\n",
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/botok/textunits/bostring.py:82: UserWarning: Beware of unexpected results: input string contains the non-expanded char \"བྷ\", found in \"\".\n",
      "  warn(\n",
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/botok/textunits/bostring.py:82: UserWarning: Beware of unexpected results: input string contains the non-expanded char \"ྦྷ\", found in \"\".\n",
      "  warn(\n",
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/botok/textunits/bostring.py:82: UserWarning: Beware of unexpected results: input string contains the non-expanded char \"དྷ\", found in \"དྷྱཱ་\".\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5 s.)\n",
      "text: \"བཀྲ་ཤིས་\"\n",
      "text_cleaned: \"བཀྲ་ཤིས་\"\n",
      "text_unaffixed: \"བཀྲ་ཤིས་\"\n",
      "syls: [\"བཀྲ\", \"ཤིས\"]\n",
      "pos: NOUN\n",
      "lemma: བཀྲ་ཤིས་\n",
      "senses: | pos: NOUN, freq: 17204, affixed: False, lemma: བཀྲ་ཤིས་ |\n",
      "char_types: |CONS|CONS|SUB_CONS|TSEK|CONS|VOW|CONS|TSEK|\n",
      "chunk_type: TEXT\n",
      "freq: 17204\n",
      "syls_idx: [[0, 1, 2], [4, 5, 6]]\n",
      "syls_start_end: [{'start': 0, 'end': 4}, {'start': 4, 'end': 8}]\n",
      "start: 0\n",
      "len: 8\n",
      "\n",
      "\n",
      "text: \"བདེ་ལེགས་\"\n",
      "text_cleaned: \"བདེ་ལེགས་\"\n",
      "text_unaffixed: \"བདེ་ལེགས་\"\n",
      "syls: [\"བདེ\", \"ལེགས\"]\n",
      "pos: NOUN\n",
      "lemma: བདེ་ལེགས་\n",
      "senses: | pos: NOUN, freq: 3918, affixed: False, lemma: བདེ་ལེགས་ |\n",
      "char_types: |CONS|CONS|VOW|TSEK|CONS|VOW|CONS|CONS|TSEK|\n",
      "chunk_type: TEXT\n",
      "freq: 3918\n",
      "syls_idx: [[0, 1, 2], [4, 5, 6, 7]]\n",
      "syls_start_end: [{'start': 0, 'end': 4}, {'start': 4, 'end': 9}]\n",
      "start: 8\n",
      "len: 9\n",
      "\n",
      "\n",
      "text: \"ཞུས་\"\n",
      "text_cleaned: \"ཞུས་\"\n",
      "text_unaffixed: \"ཞུ་\"\n",
      "syls: [\"ཞུས\"]\n",
      "pos: VERB\n",
      "lemma: ཞུ་\n",
      "senses: | pos: VERB, freq: 14265, affixed: True, lemma: ཞུ་ | pos: VERB, freq: 6404, affixed: False, lemma: ཞུ་ | pos: VERB, freq: 14265, affixed: True, lemma: ཞུ་ | pos: VERB, freq: 6404, affixed: False, lemma: ཞུ་ |\n",
      "char_types: |CONS|VOW|CONS|TSEK|\n",
      "chunk_type: TEXT\n",
      "freq: 6404\n",
      "syls_idx: [[0, 1, 2]]\n",
      "syls_start_end: [{'start': 0, 'end': 4}]\n",
      "start: 17\n",
      "len: 4\n",
      "\n",
      "\n",
      "text: \"རྒྱུ་\"\n",
      "text_cleaned: \"རྒྱུ་\"\n",
      "text_unaffixed: \"རྒྱུ་\"\n",
      "syls: [\"རྒྱུ\"]\n",
      "pos: NOUN\n",
      "lemma: རྒྱུ་\n",
      "senses: | pos: NOUN, freq: 84021, affixed: False, lemma: རྒྱུ་ |\n",
      "char_types: |CONS|SUB_CONS|SUB_CONS|VOW|TSEK|\n",
      "chunk_type: TEXT\n",
      "freq: 84021\n",
      "syls_idx: [[0, 1, 2, 3]]\n",
      "syls_start_end: [{'start': 0, 'end': 5}]\n",
      "start: 21\n",
      "len: 5\n",
      "\n",
      "\n",
      "text: \"ཡིན་ \"\n",
      "text_cleaned: \"ཡིན་\"\n",
      "text_unaffixed: \"ཡིན་\"\n",
      "syls: [\"ཡིན\"]\n",
      "pos: NO_POS\n",
      "lemma: ཡིན་\n",
      "senses: | pos: NO_POS, lemma: ཡིན་ |\n",
      "char_types: |CONS|VOW|CONS|TSEK|TRANSPARENT|\n",
      "chunk_type: TEXT\n",
      "syls_idx: [[0, 1, 2]]\n",
      "syls_start_end: [{'start': 0, 'end': 5}]\n",
      "start: 26\n",
      "len: 5\n",
      "\n",
      "\n",
      "text: \"སེམས་པ་\"\n",
      "text_cleaned: \"སེམས་པ་\"\n",
      "text_unaffixed: \"སེམས་པ་\"\n",
      "syls: [\"སེམས\", \"པ\"]\n",
      "pos: VERB\n",
      "lemma: སེམས་པ་\n",
      "senses: | pos: VERB, freq: 6274, affixed: False, lemma: སེམས་པ་ |\n",
      "char_types: |CONS|VOW|CONS|CONS|TSEK|CONS|TSEK|\n",
      "chunk_type: TEXT\n",
      "freq: 6274\n",
      "syls_idx: [[0, 1, 2, 3], [5]]\n",
      "syls_start_end: [{'start': 0, 'end': 5}, {'start': 5, 'end': 7}]\n",
      "start: 31\n",
      "len: 7\n",
      "\n",
      "\n",
      "text: \"སྐྱིད་པོ་\"\n",
      "text_cleaned: \"སྐྱིད་པོ་\"\n",
      "text_unaffixed: \"སྐྱིད་པོ་\"\n",
      "syls: [\"སྐྱིད\", \"པོ\"]\n",
      "pos: ADJ\n",
      "lemma: སྐྱིད་པོ་\n",
      "senses: | pos: ADJ, freq: 53, affixed: False, lemma: སྐྱིད་པོ་ |\n",
      "char_types: |CONS|SUB_CONS|SUB_CONS|VOW|CONS|TSEK|CONS|VOW|TSEK|\n",
      "chunk_type: TEXT\n",
      "freq: 53\n",
      "syls_idx: [[0, 1, 2, 3, 4], [6, 7]]\n",
      "syls_start_end: [{'start': 0, 'end': 6}, {'start': 6, 'end': 9}]\n",
      "start: 38\n",
      "len: 9\n",
      "\n",
      "\n",
      "text: \"འདུག\"\n",
      "text_cleaned: \"འདུག་\"\n",
      "text_unaffixed: \"འདུག་\"\n",
      "syls: [\"འདུག\"]\n",
      "pos: VERB\n",
      "lemma: འདུག་\n",
      "senses: | pos: VERB, freq: 28206, affixed: False, lemma: འདུག་ |\n",
      "char_types: |CONS|CONS|VOW|CONS|\n",
      "chunk_type: TEXT\n",
      "freq: 28206\n",
      "syls_idx: [[0, 1, 2, 3]]\n",
      "syls_start_end: [{'start': 0, 'end': 4}]\n",
      "start: 47\n",
      "len: 4\n",
      "\n",
      "\n",
      "text: \"།\"\n",
      "char_types: |NORMAL_PUNCT|\n",
      "chunk_type: PUNCT\n",
      "start: 51\n",
      "len: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from botok import WordTokenizer\n",
    "from botok.config import Config\n",
    "from pathlib import Path\n",
    "\n",
    "def get_tokens(wt, text):\n",
    "    tokens = wt.tokenize(text, split_affixes=False)\n",
    "    return tokens\n",
    "\n",
    "config = Config(dialect_name=\"general\", base_path= Path.home())\n",
    "wt = WordTokenizer(config=config)\n",
    "text = \"བཀྲ་ཤིས་བདེ་ལེགས་ཞུས་རྒྱུ་ཡིན་ སེམས་པ་སྐྱིད་པོ་འདུག།\"\n",
    "tokens = get_tokens(wt, text)\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded tokens: [32128, 32187, 32115, 32128, 3, 2, 32112, 3, 2, 1]\n",
      "Decoded text: ས ེ མ ས <unk> པ <unk></s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from tokenizers.pre_tokenizers import CharDelimiterSplit\n",
    "\n",
    "# Load pre-trained T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Tibetan characters to add\n",
    "tibetan_chars = [\n",
    "    # Consonants\n",
    "    \"ཀ\", \"ཁ\", \"ག\", \"ང\", \"ཅ\", \"ཆ\", \"ཇ\", \"ཉ\", \"ཏ\", \"ཐ\", \"ད\", \"ན\", \"པ\", \"པ\", \"ཕ\", \"བ\", \"མ\",\n",
    "    \"ཙ\", \"ཚ\", \"ཛ\", \"ཝ\", \"ཞ\", \"ཟ\", \"འ\", \"ཡ\", \"ར\", \"ལ\", \"ཤ\", \"ཥ\", \"ས\", \"ཧ\", \"ཨ\",\n",
    "\n",
    "    # Subjoined Consonants\n",
    "    \"ྐ\", \"ྑ\", \"ྒ\", \"ྒྷ\", \"ྔ\", \"ྕ\", \"ྖ\", \"ྗ\", \"྘\", \"ྙ\", \"ྚ\", \"ྛ\", \"ྜ\", \"ྜྷ\", \"ྞ\", \"ྟ\",\n",
    "    \"ྠ\", \"ྡ\", \"ྡྷ\", \"ྣ\", \"ྤ\", \"ྥ\", \"ྦ\", \"ྦྷ\", \"ྨ\", \"ྩ\", \"ྪ\", \"ྫ\", \"ྫྷ\", \"ྭ\", \"ྮ\", \"ྯ\",\n",
    "    \"ྰ\", \"ྱ\", \"ྲ\", \"ླ\", \"ྴ\", \"ྵ\", \"ྶ\", \"ྷ\", \"ྸ\", \"ྐྵ\", \"ྺ\", \"ྻ\", \"ྼ\", \"྽\", \"྾\", \"྿\",\n",
    "\n",
    "    # Vowels\n",
    "    \"ི\", \"ཱི\", \"ུ\", \"ཱུ\", \"ྲྀ\", \"ཷ\", \"ླྀ\", \"ཹ\", \"ེ\", \"ཻ\", \"ོ\", \"ཽ\", \"ཾ\", \"ཿ\",\n",
    "\n",
    "    # Other Marks and Symbols\n",
    "    \"འ\", \"ཡ\", \"ར\", \"ལ\", \"ཤ\", \"ཥ\", \"ས\", \"ཧ\", \"ཨ\",\n",
    "\n",
    "    # Additional Tibetan Characters\n",
    "    \"ཀྵ\", \"ཁྵ\", \"གྵ\", \"ངྵ\", \"ཅྵ\", \"ཆྵ\", \"ཇྵ\", \"ཉྵ\", \"ཏྵ\", \"ཐྵ\", \"དྵ\", \"ནྵ\", \"པྵ\", \n",
    "    \"པྵ\", \"ཕྵ\", \"བྵ\", \"མྵ\", \"ཙྵ\", \"ཚྵ\", \"ཛྵ\", \"ཝྵ\", \"ཞྵ\", \"ཟྵ\", \"འྵ\", \"ཡྵ\", \"རྵ\", \n",
    "    \"ལྵ\", \"ཤྵ\", \"ཥྵ\", \"སྵ\", \"ཧྵ\", \"ཨྵ\", \"པྪ\", \"པྫ\", \"པྫྷ\", \"པྭ\", \"པྮ\", \"པྯ\", \"པྰ\", \n",
    "    \"པྱ\", \"པྲ\", \"པླ\", \"པྴ\", \"པྵ\", \"པྶ\", \"པྷ\", \"པྸ\", \"པྐྵ\", \"པྺ\", \"པྻ\", \"པྼ\", \"པ྽\", \n",
    "    \"པ྾\", \"པ྿\"\n",
    "]\n",
    "\n",
    "\n",
    "#'ཀཁགངཅཆཇཉཏཐདནཔཕབམཙཚཛཝཞཟའཡརལཤཥསཧཨ'\n",
    "\n",
    "# Add the Tibetan characters to the tokenizer's vocabulary\n",
    "new_tokens = [char for char in tibetan_chars if char not in tokenizer.get_vocab()]\n",
    "\n",
    "# Add new tokens to the tokenizer\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Resize model embeddings to accommodate the new vocabulary size\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Test encoding with Tibetan text\n",
    "tibetan_text = \"སེ མ ས ་ པ་\"\n",
    "encoded = tokenizer.encode(tibetan_text, add_special_tokens=True)\n",
    "print(\"Encoded tokens:\", encoded)\n",
    "\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"Decoded text:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.pre_tokenizer = CharDelimiterSplit('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
